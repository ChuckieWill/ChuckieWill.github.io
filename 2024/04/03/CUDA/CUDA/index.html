<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="0windows的WSL安装Ubuntu">
<meta property="og:type" content="article">
<meta property="og:title" content="CUDA">
<meta property="og:url" content="http://yoursite.com/2024/04/03/CUDA/CUDA/index.html">
<meta property="og:site_name" content="Chuckie&#39;s Blog">
<meta property="og:description" content="0windows的WSL安装Ubuntu">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/718eaf077706432f95d69aef69893bc7.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240403101508627.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/49664d27030a46f3a3dbddfaa5264fcb.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240403112716392.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/7160e2a5ec8444f5b2bfcedb18bb499a.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240407111032368.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240407111135366.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240407111159952.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240407160153386.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240407160314890.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240407160434094.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240407163417718.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408101439935.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408101554264.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408101927047.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408102107787.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408102520942.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408102923127.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408103311538.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408103651528.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408111731166.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408111849545.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408111939744.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408112154382.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408114115427.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408153029562.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408153205717.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408153400019.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408160620523.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408160700580.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408160915680.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408161010197.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408163804198.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408164302887.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408171428927.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408171535056.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240408171607097.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240409112820833.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240409113205025.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240409114104567.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240410091835764.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240410091849591.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240410093553586.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240410102751661.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240410150451090.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240410150648112.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240410151421799.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240410155730093.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240410160418026.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240410161523872.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240410162455275.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240410162951128.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240410163654654.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240410163838655.png">
<meta property="og:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/image-20240410164028911.png">
<meta property="article:published_time" content="2024-04-03T02:43:17.000Z">
<meta property="article:modified_time" content="2024-09-18T15:06:07.448Z">
<meta property="article:author" content="Chuckie">
<meta property="article:tag" content="CUDA">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2024/04/03/CUDA/CUDA/718eaf077706432f95d69aef69893bc7.png">

<link rel="canonical" href="http://yoursite.com/2024/04/03/CUDA/CUDA/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>CUDA | Chuckie's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Chuckie's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/04/03/CUDA/CUDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Chuckie">
      <meta itemprop="description" content="Bright future">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chuckie's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CUDA
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-04-03 10:43:17" itemprop="dateCreated datePublished" datetime="2024-04-03T10:43:17+08:00">2024-04-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-09-18 23:06:07" itemprop="dateModified" datetime="2024-09-18T23:06:07+08:00">2024-09-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA/" itemprop="url" rel="index"><span itemprop="name">CUDA</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>44k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>40 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="0"><a href="#0" class="headerlink" title="0"></a>0</h1><h2 id="windows的WSL安装Ubuntu"><a href="#windows的WSL安装Ubuntu" class="headerlink" title="windows的WSL安装Ubuntu"></a>windows的WSL安装Ubuntu</h2><blockquote>
<p><a href="https://blog.csdn.net/qq_40102732/article/details/134992151" target="_blank" rel="noopener">Windows通过WSL安装Ubuntu以及深度学习配置</a></p>
<p><a href="https://blog.csdn.net/qq_43779149/article/details/130447527" target="_blank" rel="noopener">windows卸载wsl下的ubuntu</a></p>
<p>intel集成显卡不能使用cuda，所以使用WSL和虚拟机都不能支持CUDA</p>
</blockquote>
<p>打开cmd终端（管理员身份）</p>
<p>默认安装方式：</p>
<ul>
<li>该方式安装的是最新的ubuntu,不推荐</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl --install</span><br></pre></td></tr></table></figure>

<p>查看wsl版本</p>
<ul>
<li>wsl1、wsl2两个版本</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl --list --verbose</span><br></pre></td></tr></table></figure>

<p>卸载已经安装的版本</p>
<ul>
<li>Ubuntu是设备上安装的ubuntu名称</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl --unregister Ubuntu</span><br></pre></td></tr></table></figure>

<p>查看设备中安装的Ubuntu名称</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl --list</span><br></pre></td></tr></table></figure>

<p>选择可安装的Ubuntu版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl.exe --list --online</span><br></pre></td></tr></table></figure>

<p><img src="/2024/04/03/CUDA/CUDA/718eaf077706432f95d69aef69893bc7.png" alt="img"></p>
<p>安装指定的版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl.exe --install Ubuntu-18.04</span><br></pre></td></tr></table></figure>

<p>将系统安装转移到E盘上（默认安装是在C盘的）</p>
<ul>
<li>打包</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl --export Ubuntu-18.04 e:\wsl-ubuntu18.04.tar</span><br></pre></td></tr></table></figure>

<ul>
<li>接下来注销原来c盘的Ubuntu18.04,并解压包文件安装在e盘</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wsl --unregister Ubuntu-18.04  # 注销</span><br><span class="line">wsl --import Ubuntu-18.04 e:\wsl-ubuntu18.04 e:\wsl-ubuntu18.04.tar --version 2</span><br></pre></td></tr></table></figure>



<h1 id="1-2"><a href="#1-2" class="headerlink" title="1.2"></a>1.2</h1><h2 id="CUDA下载安装"><a href="#CUDA下载安装" class="headerlink" title="CUDA下载安装"></a>CUDA下载安装</h2><blockquote>
<p>系统：Ubuntu18.04</p>
<p>CUDA：11.6</p>
<p>下载地址：<a href="https://developer.nvidia.com/cuda-11-6-0-download-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-11-6-0-download-archive</a></p>
<p>注意：在虚拟机的ubuntu上是不能使用的，因为不能识别显卡</p>
</blockquote>
<p>下载：</p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240403101508627.png" alt="image-20240403101508627"></p>
<p>安装：</p>
<blockquote>
<p><a href="https://blog.csdn.net/weixin_57208584/article/details/135868555" target="_blank" rel="noopener">【Linux】什么是.bashrc，以及其使用方法</a></p>
</blockquote>
<ol>
<li>添加环境变量</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.bashrc</span><br></pre></td></tr></table></figure>

<ul>
<li>文件末尾添加内容如下：<ul>
<li>安装的是什么版本号11.6就对应改成什么版本号</li>
</ul>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> config cuda</span></span><br><span class="line">export LD_LABRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.6/lib64</span><br><span class="line">export PATH=$PATH:/usr/local/cuda-11.6/bin</span><br><span class="line">export CUDA HOME=$CUDA_HOME:/usr/local/cuda-11.6</span><br><span class="line">export PATH=/usr/local/cuda/bin:$PATH</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>环境变量立即生效</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>检查是否安装成功</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">nvcc -V</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 成功返回</span></span><br><span class="line">chuckie@ubuntu:/home/chuckie/WCUDA$ nvcc -V</span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2021 NVIDIA Corporation</span><br><span class="line">Built on Fri_Dec_17_18:16:03_PST_2021</span><br><span class="line">Cuda compilation tools, release 11.6, V11.6.55 #11.6即版本号</span><br><span class="line">Build cuda_11.6.r11.6/compiler.30794723_0</span><br></pre></td></tr></table></figure>

<p>使用：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">hello_from_gpu</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Hello World from the the GPU\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    hello_from_gpu&lt;&lt;&lt;<span class="number">4</span>, <span class="number">4</span>&gt;&gt;&gt;(); <span class="comment">// 4*4  16线程执行</span></span><br><span class="line">    cudaDeviceSynchronize();    <span class="comment">// 同步  cpu与gpu同步</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>编译</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc test.cu -o test</span><br></pre></td></tr></table></figure>

<ul>
<li>执行</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;test</span><br></pre></td></tr></table></figure>



<h1 id="1-3"><a href="#1-3" class="headerlink" title="1.3"></a>1.3</h1><h2 id="nvidia-smi"><a href="#nvidia-smi" class="headerlink" title="nvidia-smi"></a>nvidia-smi</h2><blockquote>
<p><a href="https://blog.csdn.net/c2793/article/details/134143566?ops_request_misc=&request_id=&biz_id=102&utm_term=CUDA%E9%A9%B1%E5%8A%A8%E5%8F%8ACUDA%20Toolkita%E7%89%88%E6%9C%AC%E5%AF%B9%E5%BA%94%E8%A1%A8&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-134143566.142^v100^pc_search_result_base4&spm=1018.2226.3001.4187" target="_blank" rel="noopener">CUDA版本和驱动版本对照表2023（包含CUDA12）</a></p>
<p><a href="https://blog.csdn.net/str1ngthen/article/details/133792936?spm=1001.2014.3001.5501" target="_blank" rel="noopener">Ubuntu 如何根据NVIDIA显卡型号确定对应的显卡驱动版本并安装</a></p>
<p><a href="https://blog.csdn.net/str1ngthen/article/details/133793250" target="_blank" rel="noopener">Ubuntu 安装CUDA</a></p>
<p>注意：虚拟机的ubuntu是不行的，因为不能识别显卡</p>
</blockquote>
<p>本教程使用的cuda版本：CUDA 11.6 Update 1</p>
<p><img src="/2024/04/03/CUDA/CUDA/49664d27030a46f3a3dbddfaa5264fcb.png" alt="img"></p>
<p>nvidia-smi参数说明</p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240403112716392.png" alt="image-20240403112716392"></p>
<p>查询gpu详细信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">查询GPU详细信息 nvidia-smi -q</span><br><span class="line">查询特定GPU详细信息 nvidia-smi -q -i 0 &#x2F;&#x2F;查看0号GPU</span><br><span class="line">显示GPU特定信息 nvidia-smi-q -i 0 -d MEMORY &#x2F;&#x2F;查看0号GPU的内存大小</span><br><span class="line">帮助命令 nvidia-smi -help</span><br></pre></td></tr></table></figure>



<h2 id="安装显卡驱动"><a href="#安装显卡驱动" class="headerlink" title="安装显卡驱动"></a>安装显卡驱动</h2><blockquote>
<p><a href="https://blog.csdn.net/str1ngthen/article/details/133792936?spm=1001.2014.3001.5501" target="_blank" rel="noopener">Ubuntu 如何根据NVIDIA显卡型号确定对应的显卡驱动版本并安装</a></p>
</blockquote>
<p>查看gpu信息</p>
<blockquote>
<p><a href="https://blog.csdn.net/weixin_38452632/article/details/136633239" target="_blank" rel="noopener">lspci详解</a></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lspci | grep -e VGA</span><br></pre></td></tr></table></figure>

<p>查看驱动列表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ubuntu-drivers devices</span><br></pre></td></tr></table></figure>

<p><img src="/2024/04/03/CUDA/CUDA/7160e2a5ec8444f5b2bfcedb18bb499a.png" alt="在这里插入图片描述"></p>
<ul>
<li><p>如果没有显示nvidia相关驱动列表，请检查显卡设置是否完整</p>
</li>
<li><p>可以看到这里推荐安装nvidia-driver-470，因此我们直接运行指令进行安装（<strong><em>recommend：建议、推荐</em></strong>）：</p>
</li>
</ul>
<p>安装驱动</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install nvidia-driver-470  #安装470驱动</span><br></pre></td></tr></table></figure>

<p>然后重启计算机</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo reboot</span><br></pre></td></tr></table></figure>

<h2 id="ubuntu安装g"><a href="#ubuntu安装g" class="headerlink" title="ubuntu安装g++"></a>ubuntu安装g++</h2><p>Ubuntu下载g++:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install g++</span><br></pre></td></tr></table></figure>

<h2 id="nvcc-fatal错误"><a href="#nvcc-fatal错误" class="headerlink" title="nvcc fatal错误"></a>nvcc fatal错误</h2><p><code>nvcc-V</code> 和<code>nvidia-smi</code>都正常，但是编译出现如下错误</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wangyj@node29:~&#x2F;cuda$ nvcc test.cu -o test</span><br><span class="line">nvcc fatal   : Path to libdevice library not specified</span><br></pre></td></tr></table></figure>

<p>还是环境变量没有设置好</p>
<p>按如下在<code>~/.bashrc</code>设置</p>
<ul>
<li>cuda-11.6改成nvcc -V显示的正确版本</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> config cuda</span></span><br><span class="line">export LD_LABRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.6/lib64</span><br><span class="line">export PATH=$PATH:/usr/local/cuda-11.6/bin</span><br><span class="line">export CUDA HOME=$CUDA_HOME:/usr/local/cuda-11.6</span><br><span class="line">export PATH=/usr/local/cuda/bin:$PATH</span><br></pre></td></tr></table></figure>

<h1 id="2-2"><a href="#2-2" class="headerlink" title="2.2"></a>2.2</h1><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>编程中通过核函数来启动GPU</p>
<p>注意事项：</p>
<p>1、核函数只能访问GPU内存</p>
<ul>
<li>GPU使用自己的显存，CPU使用自己的内存，它们之间互相不可直接访问，需要通过PCI总线交互</li>
</ul>
<p>2、核函数不能使用变长参数</p>
<p>3、核函数不能使用静态变量</p>
<p>4、核函数不能使用函数指针</p>
<p>5、核函数具有异步性</p>
<ul>
<li>核函数只是启动GPU执行，CPU不会等待GPU执行完成，需要等待</li>
</ul>
<p>6、核函数不支持C++的iostream</p>
<ul>
<li>需要使用printf</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// __global__ : 核函数的标识</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">hello_from_gpu</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Hello World from the the GPU\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 第一个1设置线程块的个数，第二个1设置一个线程块中线程的个数</span></span><br><span class="line">    hello_from_gpu&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line">    <span class="comment">// 主机与设备同步（主机就是CPU,设备就是GPU，CPU等待GPU执行完成）</span></span><br><span class="line">    cudaDeviceSynchronize();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="2-3"><a href="#2-3" class="headerlink" title="2.3"></a>2.3</h1><h2 id="线程模型"><a href="#线程模型" class="headerlink" title="线程模型"></a>线程模型</h2><blockquote>
<p>参考ppt</p>
</blockquote>
<h3 id="一维"><a href="#一维" class="headerlink" title="一维"></a>一维</h3><p>一个核函数的调用对应于一个网格（grid）</p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240407111032368.png" alt="image-20240407111032368"></p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240407111135366.png" alt="image-20240407111135366"></p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240407111159952.png" alt="image-20240407111159952"></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">hello_from_gpu</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> bid = blockIdx.x; <span class="comment">// 当前块id</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> tid = threadIdx.x; <span class="comment">// 当前线程在当前块中的id</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> id = threadIdx.x + blockIdx.x * blockDim.x; <span class="comment">// 当前线程id</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Hello World from block %d and thread %d, global id %d\n"</span>, bid, tid, id);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    hello_from_gpu&lt;&lt;&lt;<span class="number">2</span>, <span class="number">4</span>&gt;&gt;&gt;();</span><br><span class="line">    cudaDeviceSynchronize();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 打印</span></span><br><span class="line">Hello World from block <span class="number">0</span> <span class="keyword">and</span> thread <span class="number">0</span>, global id <span class="number">0</span></span><br><span class="line">Hello World from block <span class="number">0</span> <span class="keyword">and</span> thread <span class="number">1</span>, global id <span class="number">1</span></span><br><span class="line">Hello World from block <span class="number">0</span> <span class="keyword">and</span> thread <span class="number">2</span>, global id <span class="number">2</span></span><br><span class="line">Hello World from block <span class="number">0</span> <span class="keyword">and</span> thread <span class="number">3</span>, global id <span class="number">3</span></span><br><span class="line">Hello World from block <span class="number">1</span> <span class="keyword">and</span> thread <span class="number">0</span>, global id <span class="number">4</span></span><br><span class="line">Hello World from block <span class="number">1</span> <span class="keyword">and</span> thread <span class="number">1</span>, global id <span class="number">5</span></span><br><span class="line">Hello World from block <span class="number">1</span> <span class="keyword">and</span> thread <span class="number">2</span>, global id <span class="number">6</span></span><br><span class="line">Hello World from block <span class="number">1</span> <span class="keyword">and</span> thread <span class="number">3</span>, global id <span class="number">7</span></span><br></pre></td></tr></table></figure>

<h3 id="多维"><a href="#多维" class="headerlink" title="多维"></a>多维</h3><blockquote>
<p>注意:内建变量(blockldx、threadldx、gridDim、blockDim)只在核函数有效，且无需定义!</p>
<p>多维网格和多维线程块本质是一维的，GPU物理上不分块。</p>
</blockquote>
<p>1、CUDA可以组织三维的网格和线程块，<br>2、blockldx和threadldx是类型为uint3（无符号int）的变量，该类型是一个结构体，具有x,y,z三个成员(3个<br>成员都为无符号类型的成员构成)</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">blockldx.x</span><br><span class="line">blockldx.y</span><br><span class="line">blockIdx.z</span><br><span class="line">    </span><br><span class="line">threadldx.x</span><br><span class="line">threadldx.y</span><br><span class="line">threadldx.z</span><br></pre></td></tr></table></figure>

<p>3、gridDim和blockDim是类型为dim3的变量，该类型是一个结构体，具有x,y,z三个成员</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">gridDim.x</span><br><span class="line">gridDim.y</span><br><span class="line">gridDim.z</span><br><span class="line">    </span><br><span class="line">blockDim.x</span><br><span class="line">blockDim.y</span><br><span class="line">blockDim.z</span><br></pre></td></tr></table></figure>

<p>4、取值范围</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">blockldx.x范围------------[<span class="number">0</span>, gridDim.x<span class="number">-1</span>]</span><br><span class="line">blockldx.y范围------------[<span class="number">0</span>, gridDim.y<span class="number">-1</span>]</span><br><span class="line">blockldx.z范围------------[<span class="number">0</span>, gridDim.z<span class="number">-1</span>]</span><br><span class="line">    </span><br><span class="line">threadldx.x范围-----------[<span class="number">0</span>, blockDim.x<span class="number">-1</span>]</span><br><span class="line">threadldx.y范围-----------[<span class="number">0</span>, blockDim.y<span class="number">-1</span>]</span><br><span class="line">threadldx.z范围-----------[<span class="number">0</span>, blockDim.z<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>

<p>参数说明</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;&lt;&lt;grid_size,block_size&gt;&gt;&gt;</span><br><span class="line">grid_size----------------&gt;gridDim.x</span><br><span class="line">block_size---------------&gt;blockDim.x</span><br><span class="line">    </span><br><span class="line">gridDim和blockDim没有指定的维度默认为<span class="number">1</span>:</span><br><span class="line">gridDim.x = grid_size</span><br><span class="line">gridDim.y = <span class="number">1</span></span><br><span class="line">gridDim.z = <span class="number">1</span></span><br><span class="line">blockDim.x= block_size</span><br><span class="line">blockDim.y= <span class="number">1</span></span><br><span class="line">blockDim.z= <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>多维定义</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义多维网格和线程块(c++构造函数语法)</span></span><br><span class="line"><span class="function">dim3 <span class="title">grid_size</span><span class="params">(Gx, Gy, Gz)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">block_size</span><span class="params">(Bx, By, Bz)</span></span>;</span><br><span class="line"><span class="comment">// 举个例子,定义一个 2x2x1的网格，5x3x1的线程块，代码中定义如下</span></span><br><span class="line"><span class="function">dim3 <span class="title">grid_size</span><span class="params">(<span class="number">2</span>, <span class="number">2</span>)</span></span>; <span class="comment">//等价于dim3 grid_size(2,2,1);</span></span><br><span class="line"><span class="function">dim3 <span class="title">block_size</span><span class="params">(<span class="number">5</span>,<span class="number">3</span>)</span></span>; <span class="comment">//等价于dim3 block_size(5,3,1);</span></span><br></pre></td></tr></table></figure>

<p>网格和线程块的限制条件</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">网格大小限制:</span><br><span class="line">gridDim.x 最大值-------<span class="number">-2</span>^<span class="number">31</span><span class="number">-1</span></span><br><span class="line">gridDim.y 最大值-------<span class="number">-2</span>^<span class="number">16</span><span class="number">-1</span></span><br><span class="line">gridDim.z 最大值-------<span class="number">-2</span>^<span class="number">16</span><span class="number">-1</span></span><br><span class="line">    </span><br><span class="line">线程块大小限制:</span><br><span class="line">blockDim.x 最大值----------<span class="number">-1024</span></span><br><span class="line">blockDim.y 最大值----------<span class="number">-1024</span></span><br><span class="line">blockDim.z 最大值----------<span class="number">-64</span></span><br><span class="line">注意:线程块总的大小最大为<span class="number">1024</span>!!!</span><br></pre></td></tr></table></figure>

<h1 id="2-4"><a href="#2-4" class="headerlink" title="2.4"></a>2.4</h1><h2 id="线程全局索引计算方式"><a href="#线程全局索引计算方式" class="headerlink" title="线程全局索引计算方式"></a>线程全局索引计算方式</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">一维Grid 一维Block:</span><br><span class="line"><span class="keyword">int</span> blockid = blockIdx.x;</span><br><span class="line"><span class="keyword">int</span> threadid = threadIdx.x;</span><br><span class="line"><span class="keyword">int</span> id = blockIdx.x *blockDim.x + threadIdx.x;</span><br><span class="line">一维Grid 二维Block:</span><br><span class="line"><span class="keyword">int</span> blockid = blockIdx.x;</span><br><span class="line"><span class="keyword">int</span> threadid = threadIdx.y* blockDim.x + threadIdx.x</span><br><span class="line"><span class="keyword">int</span> id = blockIdx.x* blockDim.x* blockDim.y + threadIdx.y* blockDim.x + threadIdx.x;</span><br><span class="line">一维Grid 三维Block</span><br><span class="line"><span class="keyword">int</span> blockid = blockIdx.x;</span><br><span class="line"><span class="keyword">int</span> threadid = threadIdx.z*threadDim.y*threadDim.x + threadIdx.y*threadDim.x + threadIdx.x;</span><br><span class="line"><span class="keyword">int</span> id = blockIdx.x*blockDim.x* blockDim.y* blockDim.z</span><br><span class="line">         + threadIdx.z *blockDim.y* blockDim.x</span><br><span class="line">         + threadIdx.y* blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">二维Grid 一维Block:</span><br><span class="line"><span class="keyword">int</span> blockid = blockIdx.y* gridDim.x + blockIdx.x;</span><br><span class="line"><span class="keyword">int</span> id = blockid * blockDim.x + threadIdx.x;</span><br><span class="line">二维Grid 二维Block:</span><br><span class="line"><span class="keyword">int</span> blockid = blockIdx.y* gridDim.x + blockIdx.x;</span><br><span class="line"><span class="keyword">int</span> id = blockid * (blockDim.x* blockDim.y)+ (threadIdx.y* blockDim.x)+ threadIdx.x;</span><br><span class="line">二维Grid 三维Block</span><br><span class="line"><span class="keyword">int</span> blockid = blockIdx.y* gridDim.x + blockIdx.x;</span><br><span class="line"><span class="keyword">int</span> id = blockid * (blockDim.x* blockDim.y* blockDim.z)</span><br><span class="line">         +(threadIdx.z*(blockDim.x* blockDim.y))</span><br><span class="line">         +(threadIdx.y* blockDim.x)+ threadIdx.x;</span><br><span class="line"></span><br><span class="line">三维Grid 一维Block:</span><br><span class="line"><span class="keyword">int</span> blockid = blockIdx.x+ blockIdx.y * gridDim.x+ gridDim.x* gridDim.y* blockIdx.z;</span><br><span class="line"><span class="keyword">int</span> id = blockid * blockDim.x + threadIdx.x;</span><br><span class="line">三维Grid 二维Block:</span><br><span class="line"><span class="keyword">int</span> blockid = blockIdx.x+ blockIdx.y * gridDim.x+ gridDim.x* gridDim.y* blockIdx.z;</span><br><span class="line"><span class="keyword">int</span> id = blockid *(blockDim.x* blockDim.y)+ (threadIdx.y* blockDim.x)+ threadIdx.x;</span><br><span class="line">三维Grid 三维Block</span><br><span class="line"><span class="keyword">int</span> blockid = blockIdx.x+ blockIdx.y * gridDim.x+ gridDim.x* gridDim.y* blockIdx.z;</span><br><span class="line"><span class="keyword">int</span> id= blockid *(blockDim.x*blockDim.y* blockDim.z)</span><br><span class="line">       +(threadIdx.z*(blockDim.x* blockDim.y))</span><br><span class="line">       +(threadIdx.y* blockDim.x)+ threadIdx.x;</span><br></pre></td></tr></table></figure>

<h1 id="2-5"><a href="#2-5" class="headerlink" title="2.5"></a>2.5</h1><h2 id="nvcc编译流程与GPU计算能力"><a href="#nvcc编译流程与GPU计算能力" class="headerlink" title="nvcc编译流程与GPU计算能力"></a>nvcc编译流程与GPU计算能力</h2><h3 id="nvcc编译流程"><a href="#nvcc编译流程" class="headerlink" title="nvcc编译流程"></a>nvcc编译流程</h3><blockquote>
<p>官方文档：<a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html" target="_blank" rel="noopener">1. Introduction — NVIDIA CUDA Compiler Driver 12.4 documentation</a></p>
</blockquote>
<p>1、nvcc分离全部源代码为:(1)主机代码(2)设备代码</p>
<p>2、主机(Host)代码是C/C++语法，设备(device)代码是C/C++扩展语言编写</p>
<p>3、nvcc先将设备代码编译为PTX(Parallel Thread Execution)伪汇编代码，再将PTX代码编译为二进制的cubin目标代码</p>
<p>4、在将源代码编译为 PTX 代码时，需要用选项<code>-arch=compute_XY</code>指定一个虚拟架构的计算能力用以确定代码中能够使用的CUDA功能</p>
<p>5、在将PTX代码编译为cubin代码时，需要用选项<code>-code=sm_ZW</code>指定一个真实架构的计算能力，用以确定可执行文件能够使用的GPU</p>
<h3 id="PTX"><a href="#PTX" class="headerlink" title="PTX"></a>PTX</h3><blockquote>
<p>官方文档：<a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html" target="_blank" rel="noopener">1. Introduction — PTX ISA 8.4 documentation (nvidia.com)</a></p>
</blockquote>
<p>1、PTX(Parallel Thread Execution)是CUDA平台为基于GPU的通用计算而定义的虚拟机和指令集</p>
<p>2、nvcc编译命令总是使用两个体系结构:一个是虚拟的中间体系结构，另一个是实际的GPU体系结构</p>
<p>3、虚拟架构更像是对应用所需的GPU功能的声明</p>
<p>4、虚拟架构应该尽可能选择低—适配更多实际GPU；真实架构应该尽可能选择高—充分发挥GPU性能</p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240407160153386.png" alt="image-20240407160153386"></p>
<h3 id="GPU架构与计算能力"><a href="#GPU架构与计算能力" class="headerlink" title="GPU架构与计算能力"></a>GPU架构与计算能力</h3><p>1、每款GPU都有用于标识“计算能力”(compute capability的版本号</p>
<p>2、形式X.Y，X标识主版本号，Y表示次版本号</p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240407160314890.png" alt="image-20240407160314890"></p>
<p>并非GPU的计算能力越高，性能就越高</p>
<ul>
<li>性能与计算能力，显存容量，显存带宽，浮点数运算峰值都有关</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240407160434094.png" alt="image-20240407160434094"></p>
<h1 id="2-6"><a href="#2-6" class="headerlink" title="2.6"></a>2.6</h1><h2 id="CUDA程序兼容性问题"><a href="#CUDA程序兼容性问题" class="headerlink" title="CUDA程序兼容性问题"></a>CUDA程序兼容性问题</h2><h3 id="指定虚拟架构计算能力"><a href="#指定虚拟架构计算能力" class="headerlink" title="指定虚拟架构计算能力"></a>指定虚拟架构计算能力</h3><p>1、C/C++源码编译为PTX时，可以指定虚拟架构的计算能力，用来确定代码中能够使用的CUDA功能</p>
<p>2、C/C++源码转化为PTX这一步骤与GPU硬件无关</p>
<p>3、编译指令(指定虚拟架构计算能力）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-arch=compute_XY</span><br><span class="line">XY:第一个数字X代表计算能力的主版本号，第二个数字Y代表计算能力的次版本号</span><br></pre></td></tr></table></figure>

<p>4、PTX的指令只能在更高的计算能力的GPU使用</p>
<ul>
<li>指定的虚拟计算能力大于设备的GPU实际计算能力 则不能正常调用GPU（编译可通过，CPU部分可执行，但是核函数不会执行）</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 例如:</span></span><br><span class="line">nvcc helloworld.cu -o helloworld -arch=compute_61</span><br><span class="line"><span class="comment">// 编译出的可执行文件helloworld可以在计算能力&gt;=6.1的GPU上面执行，在计算能力小于6.1的GPU则不能执行GPU相关部分</span></span><br></pre></td></tr></table></figure>

<h3 id="指定真实架构计算能力"><a href="#指定真实架构计算能力" class="headerlink" title="指定真实架构计算能力"></a>指定真实架构计算能力</h3><p>1、PTX指令转化为二进制cubin代码与具体的GPU架构有关</p>
<p>2、编译指令(指定真实架构计算能力)：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-code=sm_XY</span><br><span class="line">XY:第一个数字X代表计算能力的主版本号，第二个数字Y代表计算能力的次版本号</span><br></pre></td></tr></table></figure>

<p>3、注意:</p>
<ul>
<li>二进制cubin代码，大版本之间不兼容!!!</li>
<li>指定真实架构计算能力的时候必须指定虚拟架构计算能力!!!<ul>
<li>否则编译直接报错</li>
</ul>
</li>
<li>指定的真实架构能力必须大于或等于虚拟架构能力!!!</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 真实架构能力小于虚拟架构能力 编译不通过 编译出错</span></span><br><span class="line">nvcc helloworld.cu -o helloworld -arch=compute_61 -code=sm_60</span><br></pre></td></tr></table></figure>

<p>4、真实架构可以实现低小版本到高小版本的兼容!</p>
<ul>
<li><p>若本机GPU实际计算能力为61</p>
</li>
<li><p>按60（低肖版本）编译可以支持60，61（高小版本）</p>
</li>
<li><p>按62编译则不能支持60，61</p>
</li>
</ul>
<h3 id="指定多个GPU版本编译"><a href="#指定多个GPU版本编译" class="headerlink" title="指定多个GPU版本编译"></a>指定多个GPU版本编译</h3><p>1、使得编译出来的可执行文件可以在多GPU中执行</p>
<p>2、同时指定多组计算能力:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 编译选项</span></span><br><span class="line">-gencode arch=compute_XY,code=sm_XY</span><br><span class="line"><span class="comment">// 例如:</span></span><br><span class="line">-gencode arch=compute_35,code=sm_35 <span class="comment">// 开普勒架构</span></span><br><span class="line">-gencode arch=compute_50,code=sm_50 <span class="comment">// 麦克斯韦架构</span></span><br><span class="line">-gencode arch=compute_60,code=sm_60 <span class="comment">// 帕斯卡架构</span></span><br><span class="line">-gencode arch=compute_70,code=sm_70 <span class="comment">// 伏特架构</span></span><br></pre></td></tr></table></figure>

<p>3、编译出的可执行文件包含4个二进制版本，生成的可执行文件称为胖二进制文件(fatbinary)</p>
<p>4、注意:</p>
<ul>
<li><p>执行上述指令必须CUDA版本支持7.0计算能力，否则会报错</p>
<ul>
<li>注意是CUDA版本，本机是11.6</li>
</ul>
</li>
<li><p>过多指定计算能力，会增加编译时间和可执行文件的大小</p>
</li>
</ul>
<h3 id="nvcc即时编译"><a href="#nvcc即时编译" class="headerlink" title="nvcc即时编译"></a>nvcc即时编译</h3><blockquote>
<p>使用场景：本机的GPU是帕斯卡（X=6版本），若在本本机编译，则编译的可执行文件不能在最新的安培（X=8版本）上执行，所以可以将ptx嵌入到可执行文件中，当在安培（X=8版本）上执行时，会即时将嵌入的ptx编译成可在安培（X=8版本）上执行的可执行文件并执行。该方式不能有效发挥安培（X=8版本）的性能。</p>
</blockquote>
<p>1、在运行可执行文件时，从保留的PTX代码临时编译出cubin文件</p>
<p>2、在可执行文件中保留PTX代码，nvcc编译指令指定所保留的PTX代码虚拟架构</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指令:</span></span><br><span class="line">-gencode arch=compute_XY,code=compute_XY</span><br><span class="line"><span class="comment">// 注意:</span></span><br><span class="line"><span class="comment">// (1)两个计算能力都是虚拟架构计算能力</span></span><br><span class="line"><span class="comment">// (2)两个虚拟架构计算能力必须一致</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 例如:</span></span><br><span class="line"></span><br><span class="line">-gencode=arch=compute_35,code=sm_35</span><br><span class="line">-gencode=arch=compute_50,code=sm_50</span><br><span class="line">-gencode=arch=compute_61,code=sm_61</span><br><span class="line">-gencode=arch=compute_61,code=compute_61  <span class="comment">// 这一步才是即时编译</span></span><br><span class="line"><span class="comment">// 简化:</span></span><br><span class="line">-arch=sm_XY</span><br><span class="line"><span class="comment">// 等价于 </span></span><br><span class="line">-gencode=arch=compute_61,code=sm_61</span><br><span class="line">-gencode=arch=compute_61,code=compute_61</span><br></pre></td></tr></table></figure>

<p><img src="/2024/04/03/CUDA/CUDA/image-20240407163417718.png" alt="image-20240407163417718"></p>
<h3 id="nvcc编译默认计算能力"><a href="#nvcc编译默认计算能力" class="headerlink" title="nvcc编译默认计算能力"></a>nvcc编译默认计算能力</h3><p>不同版本CUDA编译器在编译CUDA代码时，都有一个默认计算能力</p>
<ul>
<li>CUDA6.0及更早版本:  默认计算能力1.0</li>
<li>CUDA 6.5~CUDA 8.0: 默认计算能力2.0</li>
<li>CUDA 9.0~CUDA 10.2: 默认计算能力3.0</li>
<li>CUDA 11.6: 默认计算能力5.2</li>
</ul>
<p>查看本机CUDA编译默认支持的计算能力</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nvcc test.cu -ptx  <span class="comment">// 生成test.ptx</span></span><br><span class="line">cat test.ptx</span><br><span class="line"><span class="comment">// 文件中可以看到</span></span><br><span class="line"><span class="comment">// .target sm_52  表示默认支持计算能力5.2</span></span><br></pre></td></tr></table></figure>



<h1 id="3-1"><a href="#3-1" class="headerlink" title="3.1"></a>3.1</h1><h2 id="CUDA矩阵加法运算程序"><a href="#CUDA矩阵加法运算程序" class="headerlink" title="CUDA矩阵加法运算程序"></a>CUDA矩阵加法运算程序</h2><h3 id="CUDA程序基本框架"><a href="#CUDA程序基本框架" class="headerlink" title="CUDA程序基本框架"></a>CUDA程序基本框架</h3><p><img src="/2024/04/03/CUDA/CUDA/image-20240408101439935.png" alt="image-20240408101439935"></p>
<h3 id="设置GPU设备"><a href="#设置GPU设备" class="headerlink" title="设置GPU设备"></a>设置GPU设备</h3><p>1、获取GPU设备数量</p>
<ul>
<li><code>__host__</code>表示可以在主机（CPU）中调用</li>
<li><code>__device__</code>表示可以在设备（GPU）中调用</li>
<li><code>cudaError_t</code>返回错误代码（是否成功）</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> iDeviceCount=<span class="number">0</span>; <span class="comment">// 接受返回的GPU数量  若返回为0则表示没有GPU</span></span><br><span class="line">cudaGetDeviceCount(&amp;iDeviceCount);</span><br></pre></td></tr></table></figure>

<p><img src="/2024/04/03/CUDA/CUDA/image-20240408101554264.png" alt="image-20240408101554264"></p>
<p>2、设置GPU执行时使用的设备</p>
<ul>
<li>只有<code>__host__</code>所以该函数只能在主机中使用</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> iDev=<span class="number">0</span>; <span class="comment">// 0表示设置使用第0块GPU</span></span><br><span class="line">cudaSetDevice(iDev);</span><br></pre></td></tr></table></figure>

<p><img src="/2024/04/03/CUDA/CUDA/image-20240408101927047.png" alt="image-20240408101927047"></p>
<h3 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h3><p>CUDA通过内存分配、数据传递、内存初始化、内存释放进行内存管理</p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240408102107787.png" alt="image-20240408102107787"></p>
<h4 id="内存分配"><a href="#内存分配" class="headerlink" title="内存分配"></a>内存分配</h4><ul>
<li>主机分配内存:</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义</span></span><br><span class="line"><span class="function"><span class="keyword">extern</span> <span class="keyword">void</span> *<span class="title">malloc</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">int</span> num_bytes)</span></span>;</span><br><span class="line"><span class="comment">// 使用</span></span><br><span class="line"><span class="keyword">float</span> *fpHost_A;</span><br><span class="line">fpHost_A=(<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br></pre></td></tr></table></figure>

<ul>
<li>设备分配内存:</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> *fpDevice_A;</span><br><span class="line">cudaMalloc((<span class="keyword">float</span>**)&amp;fpDevice_A,nBytes); <span class="comment">// 注意是二级指针传入</span></span><br></pre></td></tr></table></figure>

<p><img src="/2024/04/03/CUDA/CUDA/image-20240408102520942.png" alt="image-20240408102520942"></p>
<h4 id="数据拷贝"><a href="#数据拷贝" class="headerlink" title="数据拷贝"></a>数据拷贝</h4><ul>
<li>主机数据拷贝:</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">memcpy</span><span class="params">(<span class="keyword">void</span> *dest, <span class="keyword">const</span> <span class="keyword">void</span> *src, <span class="built_in">size</span> tn)</span></span>; <span class="comment">// src:源地址，dest：目标地址，tn:拷贝的字节数</span></span><br><span class="line"><span class="comment">// 使用</span></span><br><span class="line"><span class="built_in">memcpy</span>((<span class="keyword">void</span>*)d,(<span class="keyword">void</span>*)s, nBytes);</span><br></pre></td></tr></table></figure>

<ul>
<li>设备数据拷贝:</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cudaMemcpy(Device_A, Host_A, nBytes, cudaMemcpyHostToHost);</span><br><span class="line"></span><br><span class="line">kind:</span><br><span class="line">cudaMemcpyHostToHost     主机-&gt;主机</span><br><span class="line">cudaMemcpyHostToDevice   主机-&gt;设备</span><br><span class="line">cudaMemcpyDeviceToHost   设备-&gt;主机</span><br><span class="line">cudaMemcpyDeviceToDevice 设备-&gt;设备</span><br><span class="line">cudaMemcpyDefault        默认方式只允许在支持统一虚拟寻址的系统上使用,自动识别以上四种情况之一</span><br></pre></td></tr></table></figure>

<p><img src="/2024/04/03/CUDA/CUDA/image-20240408102923127.png" alt="image-20240408102923127"></p>
<h4 id="内存初始化"><a href="#内存初始化" class="headerlink" title="内存初始化"></a>内存初始化</h4><ul>
<li>主机内存初始化</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义</span></span><br><span class="line"><span class="function"><span class="keyword">void</span>*<span class="title">memset</span><span class="params">(<span class="keyword">void</span> *str,<span class="keyword">int</span> c, <span class="built_in">size</span> tn)</span></span>;</span><br><span class="line"><span class="comment">// 使用</span></span><br><span class="line"><span class="built_in">memset</span>(fpHost_A,<span class="number">0</span>,nBytes);</span><br></pre></td></tr></table></figure>

<ul>
<li>设备内存初始化</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaMemset(fpDevice_A,<span class="number">0</span>,nBytes);</span><br></pre></td></tr></table></figure>



<p><img src="/2024/04/03/CUDA/CUDA/image-20240408103311538.png" alt="image-20240408103311538"></p>
<h4 id="内存释放"><a href="#内存释放" class="headerlink" title="内存释放"></a>内存释放</h4><ul>
<li>释放主机内存</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">free</span>(pHost_A);</span><br></pre></td></tr></table></figure>

<ul>
<li>释放设备内存</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaFree(pDevice_A);</span><br></pre></td></tr></table></figure>

<p><img src="/2024/04/03/CUDA/CUDA/image-20240408103651528.png" alt="image-20240408103651528"></p>
<h3 id="自定义设备函数"><a href="#自定义设备函数" class="headerlink" title="自定义设备函数"></a>自定义设备函数</h3><p>1、设备函数(device function)</p>
<p>(1)定义只能执行在GPU设备上的函数为设备函数</p>
<p>(2)设备函数只能被核函数或其他设备函数调用</p>
<p>(3)设备函数用<code>__device__</code>修饰</p>
<p>2、核函数(kernel function)</p>
<p>(1)用<code>__global__</code>修饰的函数称为核函数，一般由主机调用，在设备中执行</p>
<p>(2)<code>__global__</code>修饰符既不能和<code>__host__</code>同时使用，也不可与<code>__device__</code>同时使用</p>
<p>3、主机函数(host function)</p>
<p>(1)主机端的普通 C++函数可用<code>__host__</code>修饰</p>
<p>(2)对于主机端的函数,<code>__host__</code>修饰符可省略</p>
<p>(3)可以用<code>__host__</code>和<code>__device__</code>同时修饰一个函数减少冗余代码。编译器会针对主机和设备分别编译该函数。</p>
<h1 id="3-2"><a href="#3-2" class="headerlink" title="3.2"></a>3.2</h1><h2 id="CUDA错误检测"><a href="#CUDA错误检测" class="headerlink" title="CUDA错误检测"></a>CUDA错误检测</h2><h3 id="一、运行时API错误代码"><a href="#一、运行时API错误代码" class="headerlink" title="一、运行时API错误代码"></a>一、运行时API错误代码</h3><p><strong>CUDA</strong>运行时<strong>API</strong>大多支持返回错误代码，返回值类型为<strong>cudaError_t</strong>，<strong>CUDA</strong>运行时<strong>API</strong>成功执行，返回的错误代码为<strong>cudaSuccess</strong>，运行时<strong>API</strong>返回的执行状态值是枚举变量。</p>
<p>下图展示了部分<strong>CUDA</strong>运行时<strong>API</strong>错误代码的枚举变量：</p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240408111731166.png" alt="image-20240408111731166"></p>
<h3 id="二、错误检查函数"><a href="#二、错误检查函数" class="headerlink" title="二、错误检查函数"></a>二、错误检查函数</h3><p>一些内存管理和<strong>GPU</strong>设置的<strong>CUDA</strong>运行时<strong>API</strong>函数，包括设备内存分配函数<strong>cudaMalloc</strong>、获取<strong>GPU</strong>数量的函数<strong>cudaGetDeviceCount</strong>。每一个函数都有类型为<strong>cudaError_t</strong>的返回值，返回一个枚举变量的错误代码。</p>
<h4 id="1、涉及的运行时API函数介绍"><a href="#1、涉及的运行时API函数介绍" class="headerlink" title="1、涉及的运行时API函数介绍"></a>1、涉及的运行时API函数介绍</h4><p>（1）获取错误代码对应名称—<strong>cudaGetErrorName</strong></p>
<p>文档说明：</p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240408111849545.png" alt="image-20240408111849545"></p>
<p>作用：</p>
<p>返回错误代码对应的名称，__<strong>host</strong>__和__<strong>device</strong>__代表该运行时<strong>API</strong>可以运行在主机，也可以运行在设备。这个运行时<strong>API</strong>函数与前面我们介绍的都不一样，它的返回值是<strong>char *</strong>，返回一个字符串，传入的参数是<strong>cudaError_t</strong>类型的错误代码。比如我们假设函数返回的错误代码为<strong>1</strong>，运行<strong>cudaGetErrorName</strong>函数将返回字符串“<strong>cudaErrorInvalidValue</strong>”。</p>
<p>（2）获取错误代码描述信息—<strong>cudaGetErrorString</strong></p>
<p>文档说明：</p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240408111939744.png" alt="image-20240408111939744"></p>
<p>作用：</p>
<p>返回错误代码对应的描述信息，同<strong>cudaGetErrorName</strong>，__<strong>host</strong>__和__<strong>device</strong>__代表该运行时<strong>API</strong>可以运行在主机，也可以运行在设备。它的返回值是<strong>char *</strong>，返回一个字符串，传入的参数是<strong>cudaError_t</strong>类型的错误代码。比如我们假设函数返回的错误代码为<strong>1</strong>，运行<strong>cudaGetErrorName</strong>函数将返回字符串“<strong>invalid argument</strong>”。</p>
<h4 id="2、代码解析"><a href="#2、代码解析" class="headerlink" title="2、代码解析"></a>2、代码解析</h4><p>我们把错误检测函数定义在命名为<strong>tools/common.cuh</strong>的文件中，今后所有通用工具类的代码，我们都会放在这个文件内，代码展示：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> once</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">cudaError_t <span class="title">ErrorCheck</span><span class="params">(cudaError_t error_code, <span class="keyword">const</span> <span class="keyword">char</span>* filename, <span class="keyword">int</span> lineNumber)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (error_code != cudaSuccess)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"CUDA error:\r\ncode=%d, name=%s, description=%s\r\nfile=%s, line%d\r\n"</span>,</span><br><span class="line">                error_code, cudaGetErrorName(error_code), cudaGetErrorString(error_code), filename, lineNumber);</span><br><span class="line">        <span class="keyword">return</span> error_code;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> error_code;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>调用错误检测函数：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t error = ErrorCheck(cudaSetDevice(iDev), __FILE__, __LINE__);</span><br></pre></td></tr></table></figure>
<p>解析：</p>
<ol>
<li>预处理指令#<strong>pragma once</strong>确保当前文件在一个编译单元不被重复包含<strong>CUDA</strong>。</li>
<li>调用错误检查函数时，<strong>ErrorCheck</strong>函数的第一个参数是可以返回错误代码的<strong>CUDA</strong>运行时<strong>API</strong>函数，__<strong>FILE</strong>__和__<strong>LINE</strong>__是<strong>cpp</strong>预先定义的标识符，__<strong>FILE</strong>__用于指示本行语句所在源文件的文件名，__<strong>LINE</strong>__用于指示本行语句在源文件中的位置信息。</li>
<li>代码中<strong>if</strong>语句是用来判断待检测的运行时<strong>API</strong>函数是否发生错误，<strong>error_code==cudaSuccess</strong>则代表待检测的运行时<strong>API</strong>函数没有发生错误。只有发生错误时才会打印错误信息，报告错误代码、错误代码名字，错误代码文字描述、所在文件名字和发生错误行数。</li>
<li>错误代码函数会返回待检测运行时<strong>API</strong>函数的<strong>cudaError_t</strong>类型的错误代码。</li>
</ol>
<h4 id="3、实例演示"><a href="#3、实例演示" class="headerlink" title="3、实例演示"></a>3、实例演示</h4><p>这是一个运行时API函数有书写错误的代码，用于错误检测函数检测实验。代码的第<strong>24</strong>行，<strong>cudaMemcpy</strong>函数用于主机向设备拷贝数据，<strong>kind</strong>参数应该设置为<strong>cudaMemcpyHostToDevice</strong>，但是我故意将<strong>kind</strong>参数设置成<strong>cudaMemcpyDeviceToHost</strong>，数据传输方向有误。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"../tools/common.cuh"</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 1、分配主机内存，并初始化</span></span><br><span class="line">    <span class="keyword">float</span> *fpHost_A;</span><br><span class="line">    fpHost_A = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(<span class="number">4</span>);</span><br><span class="line">    <span class="built_in">memset</span>(fpHost_A, <span class="number">0</span>, <span class="number">4</span>);  <span class="comment">// 主机内存初始化为0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">float</span> *fpDevice_A;</span><br><span class="line">    cudaError_t error = ErrorCheck(cudaMalloc((<span class="keyword">float</span>**)&amp;fpDevice_A, <span class="number">4</span>), __FILE__, __LINE__);</span><br><span class="line">    cudaMemset(fpDevice_A, <span class="number">0</span>, <span class="number">4</span>);  <span class="comment">// 设备内存初始化为0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、数据从主机复制到设备</span></span><br><span class="line">    ErrorCheck(cudaMemcpy(fpDevice_A, fpHost_A, <span class="number">4</span>, cudaMemcpyDeviceToHost), __FILE__, __LINE__); </span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 3、释放主机与设备内存</span></span><br><span class="line">    <span class="built_in">free</span>(fpHost_A);  </span><br><span class="line">    ErrorCheck(cudaFree(fpDevice_A), __FILE__, __LINE__);</span><br><span class="line">    </span><br><span class="line">    ErrorCheck(cudaDeviceReset(), __FILE__, __LINE__);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<p>CUDA error:<br>code=1, name=cudaErrorInvalidValue, description=invalid argument<br>file=errorCheckFunction.cu, line24  </p>
<h3 id="三、检查核函数"><a href="#三、检查核函数" class="headerlink" title="三、检查核函数"></a>三、检查核函数</h3><p>错误检查函数无法捕捉调用核函数时发生的相关错误，前面也讲到过，核函数的返回值类型时<strong>void</strong>，即核函数不返回任何值。可以通过在调用核函数之后调用<strong>cudaGetLastError()</strong>函数捕捉核函数错误。</p>
<h4 id="1、涉及的运行时API函数介绍-1"><a href="#1、涉及的运行时API函数介绍-1" class="headerlink" title="1、涉及的运行时API函数介绍"></a>1、涉及的运行时API函数介绍</h4><p>获取cuda程序的最后一个错误—<strong>cudaGetLastError</strong></p>
<p>文档说明：</p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240408112154382.png" alt="image-20240408112154382"></p>
<p>作用：</p>
<p>返回<strong>CUDA</strong>程序的最后一个错误，__<strong>host</strong>__和__<strong>device</strong>__代表该运行时<strong>API</strong>可以运行在主机，也可以运行在设备。这个运行时<strong>API</strong>函数返回<strong>cudaError_t</strong>类型的错误代码。</p>
<h4 id="2、捕捉核函数错误方法"><a href="#2、捕捉核函数错误方法" class="headerlink" title="2、捕捉核函数错误方法"></a>2、捕捉核函数错误方法</h4><p>在调用核函数后，追加如下代码：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ErrorCheck(cudaGetLastError(), __FILE__, __LINE__);</span><br><span class="line">ErrorCheck(cudaDeviceSynchronize(), __FILE__, __LINE__);</span><br></pre></td></tr></table></figure>
<p>说明：</p>
<ol>
<li>第一条语句作用是捕捉第二条同步函数之前的最后一个错误。</li>
<li>第二条语句同步主机与设备，因为<strong>CPU</strong>和<strong>GPU</strong>是异构架构。</li>
</ol>
<h4 id="3、实例演示-1"><a href="#3、实例演示-1" class="headerlink" title="3、实例演示"></a>3、实例演示</h4><p>这段代码在调用核函数时，设置的线程块线程数量为<strong>2048</strong>个（超过了线程块内线程数量最大值<strong>1024</strong>个），核函数不能被成功调用，代码会发生错误，用于检查核函数的实验。</p>
<p>代码说明：</p>
<p>此代码是数组相加代码，在第<strong>98</strong>行，将线程块大小设置为<strong>2048</strong>，第<strong>101</strong>行执行调用核函数语句，因超出线程块的最大值会导致核函数调用失败，第<strong>102</strong>行捕捉截至第<strong>103</strong>行之前的最后一条错误。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"../tools/common.cuh"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="keyword">float</span> <span class="title">add</span><span class="params">(<span class="keyword">const</span> <span class="keyword">float</span> x, <span class="keyword">const</span> <span class="keyword">float</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x + y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">addFromGPU</span><span class="params">(<span class="keyword">float</span> *A, <span class="keyword">float</span> *B, <span class="keyword">float</span> *C, <span class="keyword">const</span> <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> bid = blockIdx.x;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> id = tid + bid * blockDim.x; </span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (id &gt;= N) <span class="keyword">return</span>;</span><br><span class="line">    C[id] = add(A[id], B[id]);</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initialData</span><span class="params">(<span class="keyword">float</span> *addr, <span class="keyword">int</span> elemCount)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; elemCount; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        addr[i] = (<span class="keyword">float</span>)(rand() &amp; <span class="number">0xFF</span>) / <span class="number">10.f</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 1、设置GPU设备</span></span><br><span class="line">    setGPU();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、分配主机内存和设备内存，并初始化</span></span><br><span class="line">    <span class="keyword">int</span> iElemCount = <span class="number">4096</span>;                     <span class="comment">// 设置元素数量</span></span><br><span class="line">    <span class="keyword">size_t</span> stBytesCount = iElemCount * <span class="keyword">sizeof</span>(<span class="keyword">float</span>); <span class="comment">// 字节数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// （1）分配主机内存，并初始化</span></span><br><span class="line">    <span class="keyword">float</span> *fpHost_A, *fpHost_B, *fpHost_C;</span><br><span class="line">    fpHost_A = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(stBytesCount);</span><br><span class="line">    fpHost_B = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(stBytesCount);</span><br><span class="line">    fpHost_C = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(stBytesCount);</span><br><span class="line">    <span class="keyword">if</span> (fpHost_A != <span class="literal">NULL</span> &amp;&amp; fpHost_B != <span class="literal">NULL</span> &amp;&amp; fpHost_C != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">memset</span>(fpHost_A, <span class="number">0</span>, stBytesCount);  <span class="comment">// 主机内存初始化为0</span></span><br><span class="line">        <span class="built_in">memset</span>(fpHost_B, <span class="number">0</span>, stBytesCount);</span><br><span class="line">        <span class="built_in">memset</span>(fpHost_C, <span class="number">0</span>, stBytesCount);</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Fail to allocate host memory!\n"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// （2）分配设备内存，并初始化</span></span><br><span class="line">    <span class="keyword">float</span> *fpDevice_A, *fpDevice_B, *fpDevice_C;</span><br><span class="line">    cudaMalloc((<span class="keyword">float</span>**)&amp;fpDevice_A, stBytesCount);</span><br><span class="line">    cudaMalloc((<span class="keyword">float</span>**)&amp;fpDevice_B, stBytesCount);</span><br><span class="line">    cudaMalloc((<span class="keyword">float</span>**)&amp;fpDevice_C, stBytesCount);</span><br><span class="line">    <span class="keyword">if</span> (fpDevice_A != <span class="literal">NULL</span> &amp;&amp; fpDevice_B != <span class="literal">NULL</span> &amp;&amp; fpDevice_C != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        cudaMemset(fpDevice_A, <span class="number">0</span>, stBytesCount);  <span class="comment">// 设备内存初始化为0</span></span><br><span class="line">        cudaMemset(fpDevice_B, <span class="number">0</span>, stBytesCount);</span><br><span class="line">        cudaMemset(fpDevice_C, <span class="number">0</span>, stBytesCount);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"fail to allocate memory\n"</span>);</span><br><span class="line">        <span class="built_in">free</span>(fpHost_A);</span><br><span class="line">        <span class="built_in">free</span>(fpHost_B);</span><br><span class="line">        <span class="built_in">free</span>(fpHost_C);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、初始化主机中数据</span></span><br><span class="line">    srand(<span class="number">666</span>); <span class="comment">// 设置随机种子</span></span><br><span class="line">    initialData(fpHost_A, iElemCount);</span><br><span class="line">    initialData(fpHost_B, iElemCount);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 4、数据从主机复制到设备</span></span><br><span class="line">    cudaMemcpy(fpDevice_A, fpHost_A, stBytesCount, cudaMemcpyHostToDevice); </span><br><span class="line">    cudaMemcpy(fpDevice_B, fpHost_B, stBytesCount, cudaMemcpyHostToDevice); </span><br><span class="line">    cudaMemcpy(fpDevice_C, fpHost_C, stBytesCount, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5、调用核函数在设备中进行计算</span></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">2048</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">((iElemCount + block.x - <span class="number">1</span>) / <span class="number">2048</span>)</span></span>; </span><br><span class="line"></span><br><span class="line">    addFromGPU&lt;&lt;&lt;grid, block&gt;&gt;&gt;(fpDevice_A, fpDevice_B, fpDevice_C, iElemCount);    <span class="comment">// 调用核函数</span></span><br><span class="line">    ErrorCheck(cudaGetLastError(), __FILE__, __LINE__);</span><br><span class="line">    ErrorCheck(cudaDeviceSynchronize(), __FILE__, __LINE__);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6、将计算得到的数据从设备传给主机</span></span><br><span class="line">    cudaMemcpy(fpHost_C, fpDevice_C, stBytesCount, cudaMemcpyDeviceToHost);</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++)    <span class="comment">// 打印</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"idx=%2d\tmatrix_A:%.2f\tmatrix_B:%.2f\tresult=%.2f\n"</span>, i+<span class="number">1</span>, fpHost_A[i], fpHost_B[i], fpHost_C[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 7、释放主机与设备内存</span></span><br><span class="line">    <span class="built_in">free</span>(fpHost_A);</span><br><span class="line">    <span class="built_in">free</span>(fpHost_B);</span><br><span class="line">    <span class="built_in">free</span>(fpHost_C);</span><br><span class="line">    cudaFree(fpDevice_A);</span><br><span class="line">    cudaFree(fpDevice_B);</span><br><span class="line">    cudaFree(fpDevice_C);</span><br><span class="line"></span><br><span class="line">    cudaDeviceReset();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<p>The count of GPUs is 1.set GPU 0 for computing.<br><strong><em>CUDA error:</em></strong><br><strong><em>code=9, name=cudaErrorInvalidConfiguration, description=invalid configurationargument</em></strong><br><strong><em>file=errorCheckKernel.cu, line102</em></strong><br>idx= 1  matrix_A:0.90   matrix_B:10.90  result=0.00<br>idx= 2  matrix_A:19.00  matrix_B:4.00   result=0.00<br>idx= 3  matrix_A:15.80  matrix_B:15.00  result=0.00<br>idx= 4  matrix_A:5.00   matrix_B:3.30   result=0.00<br>idx= 5  matrix_A:11.10  matrix_B:4.20   result=0.00<br>idx= 6  matrix_A:23.50  matrix_B:18.60  result=0.00<br>idx= 7  matrix_A:20.90  matrix_B:4.50   result=0.00<br>idx= 8  matrix_A:23.40  matrix_B:17.70  result=0.00<br>idx= 9  matrix_A:16.90  matrix_B:18.40  result=0.00<br>idx=10  matrix_A:7.30   matrix_B:18.30  result=0.00  </p>
<p>说明：</p>
<p>看斜体加粗部分，成功捕捉到错误。</p>
<h1 id="3-3"><a href="#3-3" class="headerlink" title="3.3"></a>3.3</h1><h2 id="CUDA计时"><a href="#CUDA计时" class="headerlink" title="CUDA计时"></a>CUDA计时</h2><h3 id="一、事件记时"><a href="#一、事件记时" class="headerlink" title="一、事件记时"></a>一、事件记时</h3><h4 id="1、记时代码解析"><a href="#1、记时代码解析" class="headerlink" title="1、记时代码解析"></a>1、记时代码解析</h4><blockquote>
<p><strong>CUDA</strong>事件记时代码如下，只需要将需要记时的代码嵌入记时代码之间：</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t start, stop;</span><br><span class="line">ErrorCheck(cudaEventCreate(&amp;start), __FILE__, __LINE__);</span><br><span class="line">ErrorCheck(cudaEventCreate(&amp;stop), __FILE__, __LINE__);</span><br><span class="line">ErrorCheck(cudaEventRecord(start), __FILE__, __LINE__);</span><br><span class="line">cudaEventQuery(start);  <span class="comment">//此处不可用错误检测函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/************************************************************</span></span><br><span class="line"><span class="comment">需要记时间的代码</span></span><br><span class="line"><span class="comment">************************************************************/</span></span><br><span class="line"></span><br><span class="line">ErrorCheck(cudaEventRecord(stop), __FILE__, __LINE__);</span><br><span class="line">ErrorCheck(cudaEventSynchronize(stop), __FILE__, __LINE__);</span><br><span class="line"><span class="keyword">float</span> elapsed_time;</span><br><span class="line">ErrorCheck(cudaEventElapsedTime(&amp;elapsed_time, start, stop), __FILE__, __LINE__);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"Time = %g ms.\n"</span>, elapsed_time);</span><br><span class="line"></span><br><span class="line">ErrorCheck(cudaEventDestroy(start), __FILE__, __LINE__);</span><br><span class="line">ErrorCheck(cudaEventDestroy(stop), __FILE__, __LINE__);</span><br></pre></td></tr></table></figure>

<p>代码解析：</p>
<ol>
<li>第1行<strong>cudaEvent_t start, stop：</strong>定义两个<strong>CUDA</strong>事件类型（<strong>cudaEvent_t</strong>）的变量；</li>
<li>第2、3行<strong>cudaEventCreate</strong>函数初始化定义的<strong>cudaEvent_t</strong>变量；</li>
<li>第4行通过<strong>cudaEventRecord</strong>函数，在需要记时的代码块之前记录代表时间开始的事件；</li>
<li>第5行<strong>cudaEventQuery</strong>函数在<strong>TCC</strong>驱动模式的<strong>GPU</strong>下可省略，但在处于<strong>WDDM</strong>驱动模式的<strong>GPU</strong>必须保留，因此，我们就一直保留这句函数即可。注意：<strong>cudaEventQuery</strong>函数不可使用错误检测函数；</li>
<li>第8行是需要记时的代码块；</li>
<li>第11行在需要记时的代码块之后记录代表时间结束的事件；</li>
<li>第12行<strong>cudaEventSynchronize</strong>函数作用是让主机等待事件<strong>stop</strong>被记录完毕；</li>
<li>第13~15行<strong>cudaEventElapsedTime</strong>函数的调用作用是计算<strong>cudaEvent_t</strong>变量<strong>start</strong>和<strong>stop</strong>时间差，记录在<strong>float</strong>变量<strong>elapsed_time</strong>中，并输出打印到屏幕上；</li>
<li>第17、18行调用<strong>cudaEventDestroy</strong>函数销毁<strong>start</strong>和<strong>stop</strong>这两个类型为<strong>cudaEvent_t</strong>的<strong>CUDA</strong>事件。</li>
</ol>
<h4 id="2、核函数记时实例演示"><a href="#2、核函数记时实例演示" class="headerlink" title="2、核函数记时实例演示"></a>2、核函数记时实例演示</h4><blockquote>
<p>此代码计算运行核函数10次的平均时间，核函数实际运行<strong>11</strong>次，由于第一次调用核函数，往往会花费更多的时间，如果将第一次记录进去，可能导致记录的时间不准确，因此忽略第一次调用核函数的时间，取<strong>10</strong>次平均值。</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"../tools/common.cuh"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> NUM_REPEATS 10</span></span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="keyword">float</span> <span class="title">add</span><span class="params">(<span class="keyword">const</span> <span class="keyword">float</span> x, <span class="keyword">const</span> <span class="keyword">float</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x + y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">addFromGPU</span><span class="params">(<span class="keyword">float</span> *A, <span class="keyword">float</span> *B, <span class="keyword">float</span> *C, <span class="keyword">const</span> <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> bid = blockIdx.x;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> id = tid + bid * blockDim.x; </span><br><span class="line">    <span class="keyword">if</span> (id &gt;= N) <span class="keyword">return</span>;</span><br><span class="line">    C[id] = add(A[id], B[id]);</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initialData</span><span class="params">(<span class="keyword">float</span> *addr, <span class="keyword">int</span> elemCount)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; elemCount; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        addr[i] = (<span class="keyword">float</span>)(rand() &amp; <span class="number">0xFF</span>) / <span class="number">10.f</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 1、设置GPU设备</span></span><br><span class="line">    setGPU();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、分配主机内存和设备内存，并初始化</span></span><br><span class="line">    <span class="keyword">int</span> iElemCount = <span class="number">4096</span>;                     <span class="comment">// 设置元素数量</span></span><br><span class="line">    <span class="keyword">size_t</span> stBytesCount = iElemCount * <span class="keyword">sizeof</span>(<span class="keyword">float</span>); <span class="comment">// 字节数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// （1）分配主机内存，并初始化</span></span><br><span class="line">    <span class="keyword">float</span> *fpHost_A, *fpHost_B, *fpHost_C;</span><br><span class="line">    fpHost_A = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(stBytesCount);</span><br><span class="line">    fpHost_B = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(stBytesCount);</span><br><span class="line">    fpHost_C = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(stBytesCount);</span><br><span class="line">    <span class="keyword">if</span> (fpHost_A != <span class="literal">NULL</span> &amp;&amp; fpHost_B != <span class="literal">NULL</span> &amp;&amp; fpHost_C != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">memset</span>(fpHost_A, <span class="number">0</span>, stBytesCount);  <span class="comment">// 主机内存初始化为0</span></span><br><span class="line">        <span class="built_in">memset</span>(fpHost_B, <span class="number">0</span>, stBytesCount);</span><br><span class="line">        <span class="built_in">memset</span>(fpHost_C, <span class="number">0</span>, stBytesCount);</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Fail to allocate host memory!\n"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// （2）分配设备内存，并初始化</span></span><br><span class="line">    <span class="keyword">float</span> *fpDevice_A, *fpDevice_B, *fpDevice_C;</span><br><span class="line">    ErrorCheck(cudaMalloc((<span class="keyword">float</span>**)&amp;fpDevice_A, stBytesCount), __FILE__, __LINE__);</span><br><span class="line">    ErrorCheck(cudaMalloc((<span class="keyword">float</span>**)&amp;fpDevice_B, stBytesCount), __FILE__, __LINE__);</span><br><span class="line">    ErrorCheck(cudaMalloc((<span class="keyword">float</span>**)&amp;fpDevice_C, stBytesCount), __FILE__, __LINE__);</span><br><span class="line">    <span class="keyword">if</span> (fpDevice_A != <span class="literal">NULL</span> &amp;&amp; fpDevice_B != <span class="literal">NULL</span> &amp;&amp; fpDevice_C != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        ErrorCheck(cudaMemset(fpDevice_A, <span class="number">0</span>, stBytesCount), __FILE__, __LINE__); <span class="comment">// 设备内存初始化为0</span></span><br><span class="line">        ErrorCheck(cudaMemset(fpDevice_B, <span class="number">0</span>, stBytesCount), __FILE__, __LINE__);</span><br><span class="line">        ErrorCheck(cudaMemset(fpDevice_C, <span class="number">0</span>, stBytesCount), __FILE__, __LINE__);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"fail to allocate memory\n"</span>);</span><br><span class="line">        <span class="built_in">free</span>(fpHost_A);</span><br><span class="line">        <span class="built_in">free</span>(fpHost_B);</span><br><span class="line">        <span class="built_in">free</span>(fpHost_C);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、初始化主机中数据</span></span><br><span class="line">    srand(<span class="number">666</span>); <span class="comment">// 设置随机种子</span></span><br><span class="line">    initialData(fpHost_A, iElemCount);</span><br><span class="line">    initialData(fpHost_B, iElemCount);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 4、数据从主机复制到设备</span></span><br><span class="line">    ErrorCheck(cudaMemcpy(fpDevice_A, fpHost_A, stBytesCount, cudaMemcpyHostToDevice), __FILE__, __LINE__); </span><br><span class="line">    ErrorCheck(cudaMemcpy(fpDevice_B, fpHost_B, stBytesCount, cudaMemcpyHostToDevice), __FILE__, __LINE__);</span><br><span class="line">    ErrorCheck(cudaMemcpy(fpDevice_C, fpHost_C, stBytesCount, cudaMemcpyHostToDevice), __FILE__, __LINE__);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5、调用核函数在设备中进行计算</span></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">((iElemCount + block.x - <span class="number">1</span>) / <span class="number">32</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span> t_sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> repeat = <span class="number">0</span>; repeat &lt;= NUM_REPEATS; ++repeat)</span><br><span class="line">    &#123;</span><br><span class="line">        cudaEvent_t start, stop;</span><br><span class="line">        ErrorCheck(cudaEventCreate(&amp;start), __FILE__, __LINE__);</span><br><span class="line">        ErrorCheck(cudaEventCreate(&amp;stop), __FILE__, __LINE__);</span><br><span class="line">        ErrorCheck(cudaEventRecord(start), __FILE__, __LINE__);</span><br><span class="line">        cudaEventQuery(start);  <span class="comment">//此处不可用错误检测函数</span></span><br><span class="line"></span><br><span class="line">        addFromGPU&lt;&lt;&lt;grid, block&gt;&gt;&gt;(fpDevice_A, fpDevice_B, fpDevice_C, iElemCount);    <span class="comment">// 调用核函数</span></span><br><span class="line"></span><br><span class="line">        ErrorCheck(cudaEventRecord(stop), __FILE__, __LINE__);</span><br><span class="line">        ErrorCheck(cudaEventSynchronize(stop), __FILE__, __LINE__);</span><br><span class="line">        <span class="keyword">float</span> elapsed_time;</span><br><span class="line">        ErrorCheck(cudaEventElapsedTime(&amp;elapsed_time, start, stop), __FILE__, __LINE__);</span><br><span class="line">        <span class="comment">// printf("Time = %g ms.\n", elapsed_time);</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (repeat &gt; <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            t_sum += elapsed_time;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ErrorCheck(cudaEventDestroy(start), __FILE__, __LINE__);</span><br><span class="line">        ErrorCheck(cudaEventDestroy(stop), __FILE__, __LINE__);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span> t_ave = t_sum / NUM_REPEATS;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Time = %g ms.\n"</span>, t_ave);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6、将计算得到的数据从设备传给主机</span></span><br><span class="line">    ErrorCheck(cudaMemcpy(fpHost_C, fpDevice_C, stBytesCount, cudaMemcpyDeviceToHost), __FILE__, __LINE__);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 7、释放主机与设备内存</span></span><br><span class="line">    <span class="built_in">free</span>(fpHost_A);</span><br><span class="line">    <span class="built_in">free</span>(fpHost_B);</span><br><span class="line">    <span class="built_in">free</span>(fpHost_C);</span><br><span class="line">    ErrorCheck(cudaFree(fpDevice_A), __FILE__, __LINE__);</span><br><span class="line">    ErrorCheck(cudaFree(fpDevice_B), __FILE__, __LINE__);</span><br><span class="line">    ErrorCheck(cudaFree(fpDevice_C), __FILE__, __LINE__);</span><br><span class="line"></span><br><span class="line">    ErrorCheck(cudaDeviceReset(), __FILE__, __LINE__);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对第103~129行代码进行说明：</p>
<ol>
<li>第111行为调用核函数代码，它包含在我们前文所描述的记录时间代码之间；</li>
<li>第119~122行，作用是计算除了第一次调用核函数的时间外，其它<strong>10</strong>次调用核函数的时间和；</li>
<li>第128、129行，计算<strong>10</strong>次调用核函数平均值，并打印输出。</li>
</ol>
<h3 id="二、nvprof性能刨析"><a href="#二、nvprof性能刨析" class="headerlink" title="二、nvprof性能刨析"></a>二、nvprof性能刨析</h3><h4 id="1、nvprof工具说明"><a href="#1、nvprof工具说明" class="headerlink" title="1、nvprof工具说明"></a>1、nvprof工具说明</h4><blockquote>
<p><strong>CUDA</strong> <strong>5.0</strong>后有一个工具叫做<strong>nvprof</strong>的命令行分析工具，<strong>nvprof</strong>是一个可执行文件。</p>
<p>如下执行命令语句，其中<strong>exe_name</strong>为可执行文件的名字。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvprof ./exe_name</span><br></pre></td></tr></table></figure>

<h4 id="2、nvprof实例演示"><a href="#2、nvprof实例演示" class="headerlink" title="2、nvprof实例演示"></a>2、nvprof实例演示</h4><p>代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"../tools/common.cuh"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> NUM_REPEATS 10</span></span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="keyword">float</span> <span class="title">add</span><span class="params">(<span class="keyword">const</span> <span class="keyword">float</span> x, <span class="keyword">const</span> <span class="keyword">float</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x + y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">addFromGPU</span><span class="params">(<span class="keyword">float</span> *A, <span class="keyword">float</span> *B, <span class="keyword">float</span> *C, <span class="keyword">const</span> <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> bid = blockIdx.x;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> id = tid + bid * blockDim.x; </span><br><span class="line">    <span class="keyword">if</span> (id &gt;= N) <span class="keyword">return</span>;</span><br><span class="line">    C[id] = add(A[id], B[id]);</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initialData</span><span class="params">(<span class="keyword">float</span> *addr, <span class="keyword">int</span> elemCount)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; elemCount; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        addr[i] = (<span class="keyword">float</span>)(rand() &amp; <span class="number">0xFF</span>) / <span class="number">10.f</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 1、设置GPU设备</span></span><br><span class="line">    setGPU();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、分配主机内存和设备内存，并初始化</span></span><br><span class="line">    <span class="keyword">int</span> iElemCount = <span class="number">4096</span>;                     <span class="comment">// 设置元素数量</span></span><br><span class="line">    <span class="keyword">size_t</span> stBytesCount = iElemCount * <span class="keyword">sizeof</span>(<span class="keyword">float</span>); <span class="comment">// 字节数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// （1）分配主机内存，并初始化</span></span><br><span class="line">    <span class="keyword">float</span> *fpHost_A, *fpHost_B, *fpHost_C;</span><br><span class="line">    fpHost_A = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(stBytesCount);</span><br><span class="line">    fpHost_B = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(stBytesCount);</span><br><span class="line">    fpHost_C = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(stBytesCount);</span><br><span class="line">    <span class="keyword">if</span> (fpHost_A != <span class="literal">NULL</span> &amp;&amp; fpHost_B != <span class="literal">NULL</span> &amp;&amp; fpHost_C != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">memset</span>(fpHost_A, <span class="number">0</span>, stBytesCount);  <span class="comment">// 主机内存初始化为0</span></span><br><span class="line">        <span class="built_in">memset</span>(fpHost_B, <span class="number">0</span>, stBytesCount);</span><br><span class="line">        <span class="built_in">memset</span>(fpHost_C, <span class="number">0</span>, stBytesCount);</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Fail to allocate host memory!\n"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// （2）分配设备内存，并初始化</span></span><br><span class="line">    <span class="keyword">float</span> *fpDevice_A, *fpDevice_B, *fpDevice_C;</span><br><span class="line">    ErrorCheck(cudaMalloc((<span class="keyword">float</span>**)&amp;fpDevice_A, stBytesCount), __FILE__, __LINE__);</span><br><span class="line">    ErrorCheck(cudaMalloc((<span class="keyword">float</span>**)&amp;fpDevice_B, stBytesCount), __FILE__, __LINE__);</span><br><span class="line">    ErrorCheck(cudaMalloc((<span class="keyword">float</span>**)&amp;fpDevice_C, stBytesCount), __FILE__, __LINE__);</span><br><span class="line">    <span class="keyword">if</span> (fpDevice_A != <span class="literal">NULL</span> &amp;&amp; fpDevice_B != <span class="literal">NULL</span> &amp;&amp; fpDevice_C != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        ErrorCheck(cudaMemset(fpDevice_A, <span class="number">0</span>, stBytesCount), __FILE__, __LINE__); <span class="comment">// 设备内存初始化为0</span></span><br><span class="line">        ErrorCheck(cudaMemset(fpDevice_B, <span class="number">0</span>, stBytesCount), __FILE__, __LINE__);</span><br><span class="line">        ErrorCheck(cudaMemset(fpDevice_C, <span class="number">0</span>, stBytesCount), __FILE__, __LINE__);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"fail to allocate memory\n"</span>);</span><br><span class="line">        <span class="built_in">free</span>(fpHost_A);</span><br><span class="line">        <span class="built_in">free</span>(fpHost_B);</span><br><span class="line">        <span class="built_in">free</span>(fpHost_C);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、初始化主机中数据</span></span><br><span class="line">    srand(<span class="number">666</span>); <span class="comment">// 设置随机种子</span></span><br><span class="line">    initialData(fpHost_A, iElemCount);</span><br><span class="line">    initialData(fpHost_B, iElemCount);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 4、数据从主机复制到设备</span></span><br><span class="line">    ErrorCheck(cudaMemcpy(fpDevice_A, fpHost_A, stBytesCount, cudaMemcpyHostToDevice), __FILE__, __LINE__); </span><br><span class="line">    ErrorCheck(cudaMemcpy(fpDevice_B, fpHost_B, stBytesCount, cudaMemcpyHostToDevice), __FILE__, __LINE__);</span><br><span class="line">    ErrorCheck(cudaMemcpy(fpDevice_C, fpHost_C, stBytesCount, cudaMemcpyHostToDevice), __FILE__, __LINE__);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5、调用核函数在设备中进行计算</span></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">((iElemCount + block.x - <span class="number">1</span>) / <span class="number">32</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    addFromGPU&lt;&lt;&lt;grid, block&gt;&gt;&gt;(fpDevice_A, fpDevice_B, fpDevice_C, iElemCount);    <span class="comment">// 调用核函数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6、将计算得到的数据从设备传给主机</span></span><br><span class="line">    ErrorCheck(cudaMemcpy(fpHost_C, fpDevice_C, stBytesCount, cudaMemcpyDeviceToHost), __FILE__, __LINE__);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 7、释放主机与设备内存</span></span><br><span class="line">    <span class="built_in">free</span>(fpHost_A);</span><br><span class="line">    <span class="built_in">free</span>(fpHost_B);</span><br><span class="line">    <span class="built_in">free</span>(fpHost_C);</span><br><span class="line">    ErrorCheck(cudaFree(fpDevice_A), __FILE__, __LINE__);</span><br><span class="line">    ErrorCheck(cudaFree(fpDevice_B), __FILE__, __LINE__);</span><br><span class="line">    ErrorCheck(cudaFree(fpDevice_C), __FILE__, __LINE__);</span><br><span class="line"></span><br><span class="line">    ErrorCheck(cudaDeviceReset(), __FILE__, __LINE__);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>假定生成可执行文件为<strong>nvprofAnalysis</strong>，执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvprof ./nvprofAnalysis</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240408114115427.png" alt="image-20240408114115427"></p>
<p>主要看<strong>GPU activities</strong>：</p>
<ol>
<li>[CUDA memcpy HtoD]：主机向设备拷贝数据花费时间占比44.98%；</li>
<li>[CUDA memset]：设备调用cudaMemset函数初始化数据占用时间占比23.76%；</li>
<li>核函数执行占比为18.81%；</li>
<li>[CUDA memcpy DtoH]：设备向主机拷贝数据花费时间占比12.14%；</li>
</ol>
<h1 id="3-4"><a href="#3-4" class="headerlink" title="3.4"></a>3.4</h1><h2 id="运行时GPU信息查询"><a href="#运行时GPU信息查询" class="headerlink" title="运行时GPU信息查询"></a>运行时GPU信息查询</h2><h3 id="一、运行时API查询GPU信息"><a href="#一、运行时API查询GPU信息" class="headerlink" title="一、运行时API查询GPU信息"></a>一、运行时API查询GPU信息</h3><h4 id="1、涉及的运行时API函数介绍-2"><a href="#1、涉及的运行时API函数介绍-2" class="headerlink" title="1、涉及的运行时API函数介绍"></a>1、涉及的运行时API函数介绍</h4><p>下图为获取<strong>GPU</strong>运行时信息的<strong>CUDA</strong>运行时<strong>API</strong>函数：</p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240408153029562.png" alt="image-20240408153029562"></p>
<p>作用：</p>
<ol>
<li>参数传入：1、事先定义的<strong>cudaDeviceProf</strong>类型的变量 2、计算机<strong>GPU</strong>的索引号；</li>
<li>该函数只能在主机中调用；</li>
</ol>
<h4 id="2、实例演示"><a href="#2、实例演示" class="headerlink" title="2、实例演示"></a>2、实例演示</h4><blockquote>
<p>通过调用<strong>cudaGetDeviceProperties</strong>运行时<strong>API</strong>打印设备信息：</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"../tools/common.cuh"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> device_id = <span class="number">0</span>;</span><br><span class="line">    ErrorCheck(cudaSetDevice(device_id), __FILE__, __LINE__);</span><br><span class="line"></span><br><span class="line">    cudaDeviceProp prop;</span><br><span class="line">    ErrorCheck(cudaGetDeviceProperties(&amp;prop, device_id), __FILE__, __LINE__);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Device id:                                 %d\n"</span>,</span><br><span class="line">        device_id);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Device name:                               %s\n"</span>,</span><br><span class="line">        prop.name);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Compute capability:                        %d.%d\n"</span>,</span><br><span class="line">        prop.major, prop.minor);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Amount of global memory:                   %g GB\n"</span>,</span><br><span class="line">        prop.totalGlobalMem / (<span class="number">1024.0</span> * <span class="number">1024</span> * <span class="number">1024</span>));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Amount of constant memory:                 %g KB\n"</span>,</span><br><span class="line">        prop.totalConstMem  / <span class="number">1024.0</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Maximum grid size:                         %d %d %d\n"</span>,</span><br><span class="line">        prop.maxGridSize[<span class="number">0</span>], </span><br><span class="line">        prop.maxGridSize[<span class="number">1</span>], prop.maxGridSize[<span class="number">2</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Maximum block size:                        %d %d %d\n"</span>,</span><br><span class="line">        prop.maxThreadsDim[<span class="number">0</span>], prop.maxThreadsDim[<span class="number">1</span>], </span><br><span class="line">        prop.maxThreadsDim[<span class="number">2</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Number of SMs:                             %d\n"</span>,</span><br><span class="line">        prop.multiProcessorCount);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Maximum amount of shared memory per block: %g KB\n"</span>,</span><br><span class="line">        prop.sharedMemPerBlock / <span class="number">1024.0</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Maximum amount of shared memory per SM:    %g KB\n"</span>,</span><br><span class="line">        prop.sharedMemPerMultiprocessor / <span class="number">1024.0</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Maximum number of registers per block:     %d K\n"</span>,</span><br><span class="line">        prop.regsPerBlock / <span class="number">1024</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Maximum number of registers per SM:        %d K\n"</span>,</span><br><span class="line">        prop.regsPerMultiprocessor / <span class="number">1024</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Maximum number of threads per block:       %d\n"</span>,</span><br><span class="line">        prop.maxThreadsPerBlock);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Maximum number of threads per SM:          %d\n"</span>,</span><br><span class="line">        prop.maxThreadsPerMultiProcessor);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240408153205717.png" alt="image-20240408153205717"></p>
<p>说明：</p>
<ol>
<li>Device id:  计算机中GPU的设备代号；</li>
<li>Device name:  显卡名字，我的显卡是Tesla P100-PCIE-16GB；</li>
<li>Compute capability: GPU计算能力，我的主版本是6，次版本是0；</li>
<li>Amount of global memory: 显卡显存大小，我的是4G的显存；</li>
<li>Amount of constant memory: 常量内存大小；</li>
<li>Maximum grid size:  最大网格大小（三个维度分别的最大值）；</li>
<li>Maximum block size:   最大线程块大小（三个维度分别的最大值）；</li>
<li>Number of SMs:  流多处理器数量；</li>
<li>Maximum amount of shared memory per block:  每个线程块最大共享内存数量；</li>
<li>Maximum amount of shared memory per SM:  每个流多处理器最大共享内存数量；</li>
<li>Maximum number of registers per block:  每个线程块最大寄存器内存数量；</li>
<li>Maximum number of registers per SM: 每个流多处理器最大寄存器内存数量；</li>
<li>Maximum number of threads per block:  每个线程块最大的线程数量；</li>
<li>Maximum number of threads per SM:   每个流多处理器最大的线程数量。</li>
</ol>
<h3 id="二、查询GPU计算核心数量"><a href="#二、查询GPU计算核心数量" class="headerlink" title="二、查询GPU计算核心数量"></a>二、查询GPU计算核心数量</h3><p>CUDA运行时API函数是无法查询GPU的核心数量的（起码我不知道要用哪一个运行时API函数），不过我在论坛上找到了怎么根据GPU架构计算核心数量的方法了，直接上代码：</p>
<p>代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"../tools/common.cuh"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getSPcores</span><span class="params">(cudaDeviceProp devProp)</span></span></span><br><span class="line"><span class="function"></span>&#123;  </span><br><span class="line">    <span class="keyword">int</span> cores = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> mp = devProp.multiProcessorCount;</span><br><span class="line">    <span class="keyword">switch</span> (devProp.major)&#123; <span class="comment">// 主版本号</span></span><br><span class="line">     <span class="keyword">case</span> <span class="number">2</span>: <span class="comment">// Fermi</span></span><br><span class="line">      <span class="keyword">if</span> (devProp.minor == <span class="number">1</span>) cores = mp * <span class="number">48</span>;</span><br><span class="line">      <span class="keyword">else</span> cores = mp * <span class="number">32</span>;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">     <span class="keyword">case</span> <span class="number">3</span>: <span class="comment">// Kepler</span></span><br><span class="line">      cores = mp * <span class="number">192</span>;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">     <span class="keyword">case</span> <span class="number">5</span>: <span class="comment">// Maxwell</span></span><br><span class="line">      cores = mp * <span class="number">128</span>;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">     <span class="keyword">case</span> <span class="number">6</span>: <span class="comment">// Pascal</span></span><br><span class="line">      <span class="keyword">if</span> ((devProp.minor == <span class="number">1</span>) || (devProp.minor == <span class="number">2</span>)) cores = mp * <span class="number">128</span>;</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (devProp.minor == <span class="number">0</span>) cores = mp * <span class="number">64</span>;</span><br><span class="line">      <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"Unknown device type\n"</span>);</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">     <span class="keyword">case</span> <span class="number">7</span>: <span class="comment">// Volta and Turing</span></span><br><span class="line">      <span class="keyword">if</span> ((devProp.minor == <span class="number">0</span>) || (devProp.minor == <span class="number">5</span>)) cores = mp * <span class="number">64</span>;</span><br><span class="line">      <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"Unknown device type\n"</span>);</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">     <span class="keyword">case</span> <span class="number">8</span>: <span class="comment">// Ampere</span></span><br><span class="line">      <span class="keyword">if</span> (devProp.minor == <span class="number">0</span>) cores = mp * <span class="number">64</span>;</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (devProp.minor == <span class="number">6</span>) cores = mp * <span class="number">128</span>;</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (devProp.minor == <span class="number">9</span>) cores = mp * <span class="number">128</span>; <span class="comment">// ada lovelace</span></span><br><span class="line">      <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"Unknown device type\n"</span>);</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">     <span class="keyword">case</span> <span class="number">9</span>: <span class="comment">// Hopper</span></span><br><span class="line">      <span class="keyword">if</span> (devProp.minor == <span class="number">0</span>) cores = mp * <span class="number">128</span>;</span><br><span class="line">      <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"Unknown device type\n"</span>);</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">     <span class="keyword">default</span>:</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"Unknown device type\n"</span>); </span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">return</span> cores;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> device_id = <span class="number">0</span>;</span><br><span class="line">    ErrorCheck(cudaSetDevice(device_id), __FILE__, __LINE__);</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    cudaDeviceProp prop;</span><br><span class="line">    ErrorCheck(cudaGetDeviceProperties(&amp;prop, device_id), __FILE__, __LINE__);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Compute cores is %d.\n"</span>, getSPcores(prop));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240408153400019.png" alt="image-20240408153400019"></p>
<h1 id="3-5"><a href="#3-5" class="headerlink" title="3.5"></a>3.5</h1><h2 id="组织线程模型"><a href="#组织线程模型" class="headerlink" title="组织线程模型"></a>组织线程模型</h2><blockquote>
<p>查看PPT和代码</p>
<p>以二维数组讲解</p>
</blockquote>
<h3 id="数据存储方式"><a href="#数据存储方式" class="headerlink" title="数据存储方式"></a>数据存储方式</h3><ul>
<li>数据在内存中是以线性、以行为主（c、c++）的方式存储</li>
<li>本例中，16x8的二维数组，在内存中以一段连续的128个地址存储该数组<ul>
<li>nx = 16</li>
<li>ny = 8</li>
</ul>
</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240408160620523.png" alt="image-20240408160620523"></p>
<h3 id="二维网格二维线程块"><a href="#二维网格二维线程块" class="headerlink" title="二维网格二维线程块"></a>二维网格二维线程块</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 线程与二维矩阵映射关系</span></span><br><span class="line">ix=threadIdx.x+blockIdx.x*blockDim.x</span><br><span class="line">iy=threadIdx.y+blockIdx.y*blockDim.y</span><br><span class="line"><span class="comment">// 线程与二维矩阵映射关系</span></span><br><span class="line">idx = iy*nx + ix</span><br></pre></td></tr></table></figure>

<p><img src="/2024/04/03/CUDA/CUDA/image-20240408160700580.png" alt="image-20240408160700580"></p>
<h3 id="二维网格一维线程块"><a href="#二维网格一维线程块" class="headerlink" title="二维网格一维线程块"></a>二维网格一维线程块</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 线程与二维矩阵映射关系</span></span><br><span class="line">ix = threadIdx.x+blockIdx.x*blockDim.x</span><br><span class="line">iy = blockIdx.y</span><br><span class="line"><span class="comment">// 线程与二维矩阵映射关系</span></span><br><span class="line">idx = iy*nx + ix</span><br></pre></td></tr></table></figure>

<p><img src="/2024/04/03/CUDA/CUDA/image-20240408160915680.png" alt="image-20240408160915680"></p>
<h3 id="一维网格一维线程块"><a href="#一维网格一维线程块" class="headerlink" title="一维网格一维线程块"></a>一维网格一维线程块</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 线程与二维矩阵映射关系</span></span><br><span class="line">ix = threadIdx.x+blockIdx.x*blockDim.x</span><br><span class="line">iy由循环进行指定</span><br><span class="line"><span class="comment">// 线程与二维矩阵映射关系</span></span><br><span class="line">idx=iy*nx+ ix</span><br></pre></td></tr></table></figure>

<p><img src="/2024/04/03/CUDA/CUDA/image-20240408161010197.png" alt="image-20240408161010197"></p>
<h1 id="4-1"><a href="#4-1" class="headerlink" title="4.1"></a>4.1</h1><h2 id="GPU硬件资源"><a href="#GPU硬件资源" class="headerlink" title="GPU硬件资源"></a>GPU硬件资源</h2><h3 id="流多处理器–SM"><a href="#流多处理器–SM" class="headerlink" title="流多处理器–SM"></a>流多处理器–SM</h3><ul>
<li><p>GPU并行性依靠流多处理器SM(streaming multiprocessor)来完成</p>
</li>
<li><p>一个GPU是由多个SM构成的，Fermi架构SM关键资源如下：</p>
</li>
</ul>
<p>1、CUDA核心(CUDA core)</p>
<p>2、共享内存/L1缓存(shared memory/L1 cache)</p>
<p>3、寄存器文件(RegisterFile)</p>
<p>4、加载和存储单元(Load/Store Units)</p>
<p>5、特殊函数单元(Special Function Unit)</p>
<p>6、Warps调度(Warps Scheduler) 线程调度</p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240408163804198.png" alt="image-20240408163804198"></p>
<ul>
<li>GPU中每个SM都可以支持数百个线程<strong>并发</strong>执行</li>
<li>以线程块block为单位，向SM分配线程块，多个线程块可被同时分配到一个可用的SM上</li>
<li>当一个线程块被分配好SM后，就不可以再分配到其他SM上了</li>
</ul>
<h3 id="线程模型与物理结构"><a href="#线程模型与物理结构" class="headerlink" title="线程模型与物理结构"></a>线程模型与物理结构</h3><ul>
<li>左图线程模型，是在逻辑角度进行分析</li>
<li>线程模型可以定义成干上万个线程</li>
<li>网格中的所有线程块需要分配到SM上进行执行</li>
<li>线程块内的所有线程分配到同一个SM中执行，但是每个SM上可以被分配多个线程块</li>
<li>线程块分配到SM中后，会以32个线程为一组进行分割，每个组成为一个wrap</li>
<li>右图物理结构，是在硬件角度进行分析，因为硬件资源是有限的，所以活跃的线程束的数量会受到SM资源限制。</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240408164302887.png" alt="image-20240408164302887"></p>
<h3 id="线程束"><a href="#线程束" class="headerlink" title="线程束"></a>线程束</h3><ul>
<li>CUDA 采用单指令多线程SIMT架构管理执行线程，每32个为一组，构成一个线程束。</li>
<li>果一个线程块中相邻的 32个线程构成一个线程束<ul>
<li>具体地说，一个线程块中第0到第 31 个线程属于第 0 个线程束</li>
<li>第 32 到第 63 个线程 属于第1个线程束，依此类推。</li>
</ul>
</li>
<li>每个线程束中只能包含同一线程块中的线程</li>
<li>每个线程束包含32个线程</li>
<li>线程束是GPU硬件上真正的做到了<strong>并行</strong></li>
<li>线程束数量=ceil(线程块中的线程数/32)<ul>
<li>线程块中的线程数最好是32的倍数，这样资源利用率更高</li>
</ul>
</li>
</ul>
<h1 id="4-2"><a href="#4-2" class="headerlink" title="4.2"></a>4.2</h1><h2 id="CUDA内存模型概述"><a href="#CUDA内存模型概述" class="headerlink" title="CUDA内存模型概述"></a>CUDA内存模型概述</h2><h3 id="内存结构层次特点"><a href="#内存结构层次特点" class="headerlink" title="内存结构层次特点"></a>内存结构层次特点</h3><ul>
<li><p>局部性原则</p>
<ul>
<li>时间局部性</li>
<li>空间局部性</li>
</ul>
</li>
<li><p>如图，底部存储器特点:<br>1、更低的每比特位平均成本<br>2、更高的容量<br>3、更高的延迟<br>4、更低的处理器访问频率</p>
</li>
<li><p>CPU和GPU主存采用DRAM(动态随机存取存储器<br>低延迟的内存采用SRAM(静态随机存取存储器)</p>
</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240408171428927.png" alt="image-20240408171428927"></p>
<h3 id="CUDA内存模型"><a href="#CUDA内存模型" class="headerlink" title="CUDA内存模型"></a>CUDA内存模型</h3><ul>
<li>寄存器(register)</li>
<li>共享内存(shared memory)</li>
<li>本地（局部）内存(local memory )</li>
<li>常量内存(constant memory)</li>
<li>纹理内存(texture memory)</li>
<li>全局内存(global memory)</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240408171535056.png" alt="image-20240408171535056"></p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240408171607097.png" alt="image-20240408171607097"></p>
<h1 id="4-3"><a href="#4-3" class="headerlink" title="4.3"></a>4.3</h1><h2 id="寄存器和本地内存"><a href="#寄存器和本地内存" class="headerlink" title="寄存器和本地内存"></a>寄存器和本地内存</h2><h3 id="寄存器"><a href="#寄存器" class="headerlink" title="寄存器"></a>寄存器</h3><ul>
<li><p>寄存器内存在片上(on-chip)，具有GPU上最快的访问速度，但是数量有限，属于GPU的稀缺资源;</p>
</li>
<li><p>寄存器仅可在线程内可见，生命周期也与所属线程一致;</p>
</li>
<li><p>核函数中定义的不加任何限定符的变量一般存放在寄存器中，</p>
</li>
<li><p><strong>内建变量存放于寄存器中，如gridDim、blockDim、 blockldx等</strong></p>
</li>
<li><p>核函数中定义的不加任何限定符的数组有可能存在于寄存器中，但也有可能存在于本地内存中，取决于数组大小，数组过大，寄存器放不下，则放在本地内存中</p>
</li>
<li><p>寄存器都是32位的，保存1个double类型的数据需要两个寄存器，寄存器保存在SM的寄存器文件;</p>
</li>
<li><p>计算能力5.0~9.0的GPU，每个SM中都是64K的寄存器数量，Fermi架构只有32K;</p>
</li>
<li><p>每个线程块使用的最大数量不同架构是不同的计算能力6.1是64K;</p>
</li>
<li><p>每个线程的最大寄存器数量是255个，Fermi架构是63个;</p>
</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240409112820833.png" alt="image-20240409112820833"></p>
<h3 id="本地内存"><a href="#本地内存" class="headerlink" title="本地内存"></a>本地内存</h3><ul>
<li>寄存器放不下的内存会存放在本地内存<ul>
<li>1、索引值不能在编译时确定的数组存放于本地内存:</li>
<li>2、可能占用大量寄存器空间的较大本地结构体和数组;</li>
<li>3、任何不满足核函数寄存器限定条件的变量</li>
</ul>
</li>
<li>每个线程最多高达可使用512KB的本地内存</li>
<li>本地内存从硬件角度看只是全局内存的一部分，延迟也很高，本地内存的过多使用，会减低程序的性能。</li>
<li>对于计算能力2.0以上的设备，本地内存的数据存储在每个SM的一级缓存和设备的二级缓存中</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240409113205025.png" alt="image-20240409113205025"></p>
<h3 id="寄存器溢出"><a href="#寄存器溢出" class="headerlink" title="寄存器溢出"></a>寄存器溢出</h3><ul>
<li>核函数所需的寄存器数量超出硬件设备支持，数据则会保存到本地内存(local memory)中:<ul>
<li>1、一个SM运行并行运行多个线程块/线程束，总的需求寄存器容量大于64KB;</li>
<li>2、单个线程运行所需寄存器数量大于255个!</li>
</ul>
</li>
<li>寄存器溢出会降低程序运行性能<ul>
<li>1、本地内存只是全局内存的一部分，延迟较高!</li>
<li>2、寄存器溢出的部分也可进入GPU的缓存中,</li>
</ul>
</li>
</ul>
<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><blockquote>
<p>4.3节对应代码</p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc --resource-usage registerNum.cu -o registerNum -arch=sm_60</span><br></pre></td></tr></table></figure>

<ul>
<li><code>--resource-usage</code>: 这是一个选项，用于指示 <code>nvcc</code> 输出关于资源使用情况的信息，包括寄存器数、常量内存使用量等。在编译完成后，<code>nvcc</code> 将输出关于资源使用情况的报告。</li>
<li>不加<code>-arch=sm_60</code>也可以，默认就是CUDA支持的默认计算能力（本机CUDA11.1默认为sm_52）</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240409114104567.png" alt="image-20240409114104567"></p>
<ol>
<li><strong>ptxas info: 0 bytes gmem</strong>： 这一行指示了全局内存（静态全局内存）使用情况。在这里显示的 <code>0 bytes gmem</code> 表示编译后的代码没有使用全局内存（其实代码中使用了动态全局内存）。</li>
<li><strong>ptxas info: Compiling entry function ‘_Z9addMatrixPiS_S_ii’ for ‘sm_60’</strong>： 这是 ptxas 编译器的信息。它显示了正在编译的 CUDA 核函数的名称 <code>_Z9addMatrixPiS_S_ii</code> 和目标 GPU 的架构版本 <code>sm_60</code>。在这里，<code>_Z9addMatrixPiS_S_ii</code> 是一个核函数的名称，<code>sm_60</code> 是针对 Compute Capability 6.0 的编译。</li>
<li><strong>ptxas info: Function properties for _Z9addMatrixPiS_S_ii</strong>： 这一行提供了有关特定核函数的一些属性信息。</li>
<li><strong>0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads</strong>： 这部分提供了有关核函数的栈帧、溢出存储和溢出加载的信息。在这里，显示的都是 0 字节，表示该核函数没有栈帧，也没有发生溢出存储和溢出加载。</li>
<li><strong>ptxas info: Used 8 registers, 352 bytes cmem[0]</strong>： 这一行提供了有关核函数使用的寄存器和常量内存的信息。在这里，显示使用了 8 个寄存器和 352 字节的常量内存。</li>
</ol>
<h1 id="4-4"><a href="#4-4" class="headerlink" title="4.4"></a>4.4</h1><h2 id="全局内存"><a href="#全局内存" class="headerlink" title="全局内存"></a>全局内存</h2><ul>
<li>全局内存在片外。<ul>
<li>特点:容量最大，延迟最大，使用最多</li>
</ul>
</li>
<li>全局内存中的数据所有线程可见，Host端可见且具有与程序相同的生命周期</li>
</ul>
<h3 id="全局变量初始化"><a href="#全局变量初始化" class="headerlink" title="全局变量初始化"></a>全局变量初始化</h3><ul>
<li><p>动态全局内存</p>
<ul>
<li><p>主机代码中使用CUDA运行时API <code>cudaMalloc</code>动态声明内存空间，由<code>cudaFree</code>释放全局内存</p>
</li>
<li><p>主机与<strong>GPU动态全局变量之间的交互</strong>使用<code>cudaMemcpy</code>, 参考3.1</p>
</li>
</ul>
</li>
<li><p>静态全局内存:</p>
<ul>
<li>使用<code>__device__</code>关键字静态声明全局内存</li>
<li>不能定义在函数（核函数核和主机函数）中</li>
<li>核函数可以直接访问静态全局变量，不用数据传递</li>
</ul>
</li>
<li><p>host不能直接访问GPU静态全局变量，需要通过如下两个函数和<strong>GPU静态全局变量交互</strong></p>
</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240410091835764.png" alt="image-20240410091835764"></p>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240410091849591.png" alt="image-20240410091849591"></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"common.cuh"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 静态全局变量， 不能定义在核函数中也不能定义在主机函数中</span></span><br><span class="line">__device__ <span class="keyword">int</span> d_x = <span class="number">1</span>;</span><br><span class="line">__device__ <span class="keyword">int</span> d_y[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 核函数中可以直接访问核修改静态全局变量</span></span><br><span class="line">    d_y[<span class="number">0</span>] += d_x;</span><br><span class="line">    d_y[<span class="number">1</span>] += d_x;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"d_x = %d, d_y[0] = %d, d_y[1] = %d.\n"</span>, d_x, d_y[<span class="number">0</span>], d_y[<span class="number">1</span>]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> devID = <span class="number">0</span>;</span><br><span class="line">    cudaDeviceProp deviceProps;</span><br><span class="line">    CUDA_CHECK(cudaGetDeviceProperties(&amp;deviceProps, devID));</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"运行GPU设备:"</span> &lt;&lt; deviceProps.name &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> h_y[<span class="number">2</span>] = &#123;<span class="number">10</span>, <span class="number">20</span>&#125;;</span><br><span class="line">    <span class="comment">// 将主机数据传递给GPU静态全局变量</span></span><br><span class="line">    CUDA_CHECK(cudaMemcpyToSymbol(d_y, h_y, <span class="keyword">sizeof</span>(<span class="keyword">int</span>) * <span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">1</span>)</span></span>;</span><br><span class="line">    kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;();</span><br><span class="line">    CUDA_CHECK(cudaDeviceSynchronize());</span><br><span class="line">    <span class="comment">// 将GPU静态全局变量传递给主机</span></span><br><span class="line">    CUDA_CHECK(cudaMemcpyFromSymbol(h_y, d_y, <span class="keyword">sizeof</span>(<span class="keyword">int</span>) * <span class="number">2</span>));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"h_y[0] = %d, h_y[1] = %d.\n"</span>, h_y[<span class="number">0</span>], h_y[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">    CUDA_CHECK(cudaDeviceReset());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="4-5"><a href="#4-5" class="headerlink" title="4.5"></a>4.5</h1><h2 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h2><h3 id="共享内存作用"><a href="#共享内存作用" class="headerlink" title="共享内存作用"></a>共享内存作用</h3><ul>
<li><p>共享内存在片上(on-chip)，与本地内存和全局内存相比具有更高的带宽和更低的延迟</p>
</li>
<li><p><strong>共享内存中的数据在线程块内所有线程可见</strong>，可用作线程间通信，共享内存的生命周期也与所属线程块一致</p>
</li>
<li><p>使用shared 修饰的变量存放于共享内存中，共享内存可定义动态与静态两种</p>
</li>
<li><p>每个SM的共享内存数量是一定的，也就是说，如果在单个线程块中分配过度的共享内存，将会限制活跃线程束的数量</p>
</li>
<li><p>访问共享内存必须加入同步机制:线程块内同步<code>void dsyncthreads()</code></p>
</li>
<li><p>不同计算能力的架构，每个SM中拥有的共享内存大小是不同的</p>
</li>
<li><p>每个线程块使用的最大数量不同架构是不同的</p>
<ul>
<li>以计算能力8.9为例<ul>
<li>每个SM最大有100KB的共享内存</li>
<li>每个线程块最大有99KB的共享内存，但是多个线程块的共享内存和是100KB，所以其中一个线程块有99KB的共享内存，那么其它线程块就只能有1KB的共享内存了</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240410093553586.png" alt="image-20240410093553586"></p>
<ul>
<li>经常访问的数据由全局内存(global memory)搬移到共享内存(shared memory)，提高访问效率</li>
<li>改变全局内存访问内存的内存事务方式，提高数据访问的带宽，</li>
</ul>
<h3 id="静态共享内存"><a href="#静态共享内存" class="headerlink" title="静态共享内存"></a>静态共享内存</h3><ul>
<li>共享内存变量修饰符:<code>__shared__</code></li>
<li>静态共享内存声明:<code>__shared__ float tile[size, size]</code></li>
<li>静态共享内存作用域:<br>1、核函数中声明，静态共享内存作用域局限在这个核函数中<br>2、文件核函数外声明，静态共享内存作用域对所有核函数有效</li>
<li>静态共享内存在编译时就要确定内存大小</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"common.cuh"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel_1</span><span class="params">(<span class="keyword">float</span>* d_A, <span class="keyword">const</span> <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> bid = blockIdx.x;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> n = bid * blockDim.x + tid;</span><br><span class="line">    <span class="comment">// 静态共享内存</span></span><br><span class="line">    __shared__ <span class="keyword">float</span> s_array[<span class="number">32</span>];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (n &lt; N)</span><br><span class="line">    &#123;</span><br><span class="line">        s_array[tid] = d_A[n];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 线程块内同步</span></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// tid == 0 要求只在每个线程块中的第一个线程中做如下操作</span></span><br><span class="line">    <span class="comment">// 这里应该是在线程同步之后才执行，所以在第一个线程中也能打印出其它线程赋值之后的值</span></span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">32</span>; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"kernel_1: %f, blockIdx: %d\n"</span>, s_array[i], bid);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> devID = <span class="number">0</span>;</span><br><span class="line">    cudaDeviceProp deviceProps;</span><br><span class="line">    CUDA_CHECK(cudaGetDeviceProperties(&amp;deviceProps, devID));</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"运行GPU设备:"</span> &lt;&lt; deviceProps.name &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> nElems = <span class="number">64</span>;</span><br><span class="line">    <span class="keyword">int</span> nbytes = nElems * <span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span>* h_A = <span class="literal">nullptr</span>;</span><br><span class="line">    h_A = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nbytes);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nElems; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        h_A[i] = <span class="keyword">float</span>(i);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span>* d_A = <span class="literal">nullptr</span>;</span><br><span class="line">    CUDA_CHECK(cudaMalloc(&amp;d_A, nbytes));</span><br><span class="line">    CUDA_CHECK(cudaMemcpy(d_A, h_A, nbytes,cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">2</span>)</span></span>;</span><br><span class="line">    kernel_1&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_A, nElems);</span><br><span class="line"></span><br><span class="line">    CUDA_CHECK(cudaFree(d_A));</span><br><span class="line">    <span class="built_in">free</span>(h_A);</span><br><span class="line">    CUDA_CHECK(cudaDeviceReset());</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="动态共享内存"><a href="#动态共享内存" class="headerlink" title="动态共享内存"></a>动态共享内存</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"common.cuh"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义动态共享内存， 必须要有关键字extern，数组必须是[],不能使用指针来定义数字（float *s_array 定义数组是错误的）</span></span><br><span class="line"><span class="keyword">extern</span> __shared__ <span class="keyword">float</span> s_array[];</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel_1</span><span class="params">(<span class="keyword">float</span>* d_A, <span class="keyword">const</span> <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> bid = blockIdx.x;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> n = bid * blockDim.x + tid;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (n &lt; N)</span><br><span class="line">    &#123;</span><br><span class="line">        s_array[tid] = d_A[n];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">32</span>; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"kernel_1: %f, blockIdx: %d\n"</span>, s_array[i], bid);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> devID = <span class="number">0</span>;</span><br><span class="line">    cudaDeviceProp deviceProps;</span><br><span class="line">    CUDA_CHECK(cudaGetDeviceProperties(&amp;deviceProps, devID));</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"运行GPU设备:"</span> &lt;&lt; deviceProps.name &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> nElems = <span class="number">64</span>;</span><br><span class="line">    <span class="keyword">int</span> nbytes = nElems * <span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span>* h_A = <span class="literal">nullptr</span>;</span><br><span class="line">    h_A = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nbytes);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nElems; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        h_A[i] = <span class="keyword">float</span>(i);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span>* d_A = <span class="literal">nullptr</span>;</span><br><span class="line">    CUDA_CHECK(cudaMalloc(&amp;d_A, nbytes));</span><br><span class="line">    CUDA_CHECK(cudaMemcpy(d_A, h_A, nbytes,cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">2</span>)</span></span>;</span><br><span class="line">    <span class="comment">// 注意传入的32,用来指定动态共享内存数组的大小</span></span><br><span class="line">    kernel_1&lt;&lt;&lt;grid, block, <span class="number">32</span>&gt;&gt;&gt;(d_A, nElems);</span><br><span class="line"></span><br><span class="line">    CUDA_CHECK(cudaFree(d_A));</span><br><span class="line">    <span class="built_in">free</span>(h_A);</span><br><span class="line">    CUDA_CHECK(cudaDeviceReset());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="4-6"><a href="#4-6" class="headerlink" title="4.6"></a>4.6</h1><h2 id="常量内存"><a href="#常量内存" class="headerlink" title="常量内存"></a>常量内存</h2><ul>
<li><p>常量内存是有常量缓存的全局内存，数量有限大小仅为64KB，由于有缓存，线程束在读取相同的常量内存数据时，访问速度比全局内存快</p>
</li>
<li><p>常量内存中的数据对同一编译单元内所有线程可见;</p>
</li>
<li><p>使用:<code>__constant__</code> 修饰的变量存放于常量内存中，<strong>不能定义在核函数和主机函数中(否则编译报错)</strong>，且<strong>常量内存是静态定义的(定义即需初始化)</strong>;</p>
</li>
<li><p>常量内存仅可读，不可写，</p>
</li>
<li><p>给核函数传递数值参数时，这个变量就存放于常量内存。</p>
</li>
<li><p><strong>常量内存必须在主机端使用<code>cudaMemcpyToSymbol</code>进行初始化，或者定义时就手动初始化</strong>;</p>
</li>
<li><p>线程束中所有线程从相同内存地址中读取数据时，常量内存表现最好，例如数学公式中的系数，因为线程束中所有的线程都需要读取同一个地址空间的系数数据，因此<strong>只需要读取一次，广播给线程束中的所有线程</strong>。</p>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"common.cuh"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义常量内存</span></span><br><span class="line">__constant__ <span class="keyword">float</span> c_data;</span><br><span class="line">__constant__ <span class="keyword">float</span> c_data2 = <span class="number">6.6f</span>;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel_1</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Constant data c_data = %.2f.\n"</span>, c_data);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里的参数N存储为常量内存，线程束中只用一次访问即可广播到所有线程</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel_2</span><span class="params">(<span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> idx = threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (idx &lt; N)</span><br><span class="line">    &#123;</span><br><span class="line"></span><br><span class="line">    &#125;   </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> devID = <span class="number">0</span>;</span><br><span class="line">    cudaDeviceProp deviceProps;</span><br><span class="line">    CUDA_CHECK(cudaGetDeviceProperties(&amp;deviceProps, devID));</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"运行GPU设备:"</span> &lt;&lt; deviceProps.name &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span> h_data = <span class="number">8.8f</span>;</span><br><span class="line">    <span class="comment">// 用h_data初始化常量内存c_data</span></span><br><span class="line">    CUDA_CHECK(cudaMemcpyToSymbol(c_data, &amp;h_data, <span class="keyword">sizeof</span>(<span class="keyword">float</span>)));</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">1</span>)</span></span>;</span><br><span class="line">    kernel_1&lt;&lt;&lt;grid, block&gt;&gt;&gt;();</span><br><span class="line">    CUDA_CHECK(cudaDeviceSynchronize());</span><br><span class="line">    <span class="comment">// 将常量内存c_data2赋值给主机的h_data</span></span><br><span class="line">    CUDA_CHECK(cudaMemcpyFromSymbol(&amp;h_data, c_data2, <span class="keyword">sizeof</span>(<span class="keyword">float</span>)));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Constant data h_data = %.2f.\n"</span>, h_data);</span><br><span class="line"></span><br><span class="line">    CUDA_CHECK(cudaDeviceReset());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="4-7"><a href="#4-7" class="headerlink" title="4.7"></a>4.7</h1><h2 id="GPU缓存"><a href="#GPU缓存" class="headerlink" title="GPU缓存"></a>GPU缓存</h2><h3 id="GPU缓存种类"><a href="#GPU缓存种类" class="headerlink" title="GPU缓存种类"></a>GPU缓存种类</h3><ul>
<li>一级缓存(L1);</li>
<li>二级缓存(L2);</li>
<li>只读常量缓存，</li>
<li>只读纹理缓存;</li>
</ul>
<h3 id="GPU缓存作用"><a href="#GPU缓存作用" class="headerlink" title="GPU缓存作用"></a>GPU缓存作用</h3><ul>
<li>GPU缓存是不可编程的内存:</li>
<li>每个SM都有一个一级缓存，所有SM共享一个二级缓存;</li>
<li>L1缓存和L2缓存用来存储本地内存(localmemory)和全局内存(global memory)的数据，也包括寄存器溢出的部分，<ul>
<li>如下图全局内存和本地内存都在DRAM上</li>
</ul>
</li>
<li>在GPU上只有内存加载可以被缓存，内存存储操作不能被缓存<ul>
<li>从DRAM上加载全局内存和本地内存时，可以通过L2-&gt;L1缓存，但是计算完的数据存储到DRAM时，不会通过L1，L2缓存，而是直接存储到DRAM</li>
</ul>
</li>
<li>每个SM有一个只读常量缓存和只读纹理缓存，它们用于在设备内存中提高来自各自内存空间内的读取性能。</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240410102751661.png" alt="image-20240410102751661"></p>
<h3 id="L1缓存查询与设置"><a href="#L1缓存查询与设置" class="headerlink" title="L1缓存查询与设置"></a>L1缓存查询与设置</h3><ul>
<li>GPU全局内存是否支持L1缓存查询指令<ul>
<li><code>cudaDeviceProp::globalL1CacheSupported</code></li>
</ul>
</li>
<li>默认情况下，数据不会缓存在统一的L1纹理缓存中(默认直接从L2到计算，不通过L1)，但可以通过编译指令启用缓存<ul>
<li>开启:<code>-Xptxas -dlcm=ca</code>  作为编译命令添加<ul>
<li>除了带有禁用缓存修饰符的内联汇编修饰的数据外，所有读取都将被缓存</li>
</ul>
</li>
<li>开启:<code>-Xptxas -fscm=ca</code> 作为编译命令添加<ul>
<li>所有数据读取都将被缓存。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="L1缓存与共享内存"><a href="#L1缓存与共享内存" class="headerlink" title="L1缓存与共享内存"></a>L1缓存与共享内存</h3><ul>
<li><p>计算能力为8.9的显卡为例:</p>
<ul>
<li><p>1)统一数据缓存大小为128KB，统一数据缓存<strong>包括共享内存、纹理内存和L1缓存</strong>;</p>
<ul>
<li>128KB是物理设备的大小，共享内存、纹理内存和L1缓存是逻辑概念，这三个一起分享这块物理设备</li>
</ul>
</li>
<li><p>2)共享内存从统一的数据缓存中分区出来，并且可以配置为各种大小，共享内存容量可设置为0，8，16，32，64和100KB，剩下的数据缓存用作L1缓存，也可由纹理单元使用</p>
</li>
<li><p>3)L1缓存与共享内存大小是可以进行配置的，但不一定生效，GPU会自动选择最优的配置。</p>
</li>
</ul>
</li>
<li><p>伏特架构(计算能力7.0):统一数据缓存大小为128KB，共享内存容量可以设置为0、8、16、32、64或96KB;</p>
</li>
<li><p>图灵架构(计算能力7.5):统一数据缓存大小为96KB，共享内存容量可以设置为32KB或64KB;</p>
</li>
<li><p>安培架构(计算能力8.0):统一数据缓存大小为192KB，共享内存容量可以设置为0、8、16、32、64、100、132或164KB.</p>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"common.cuh"</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> devID = <span class="number">0</span>;</span><br><span class="line">    cudaDeviceProp deviceProps;</span><br><span class="line">    CUDA_CHECK(cudaGetDeviceProperties(&amp;deviceProps, devID));</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"运行GPU设备:"</span> &lt;&lt; deviceProps.name &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 查看是否支持L1缓存</span></span><br><span class="line">    <span class="keyword">if</span> (deviceProps.globalL1CacheSupported)&#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"支持全局内存L1缓存"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"不支持全局内存L1缓存"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 查看L2缓存大小</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"L2缓存大小："</span> &lt;&lt; deviceProps.l2CacheSize / (<span class="number">1024</span> * <span class="number">1024</span>) &lt;&lt; <span class="string">"M"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">1</span>)</span></span>;</span><br><span class="line">    kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;();</span><br><span class="line">    CUDA_CHECK(cudaDeviceSynchronize());</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    CUDA_CHECK(cudaDeviceReset());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><code>-Xptxas -dlcm=ca</code>:开启L1缓存</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wangyj@node29:/home/CUDA-code/4.7lesson$ nvcc GPU_cache.cu -o GPU_cache -arch=sm_60 -Xptxas -dlcm=ca</span><br><span class="line">wangyj@node29:/home/CUDA-code/4.7lesson$ ./GPU_cache </span><br><span class="line">运行GPU设备:Tesla P100-PCIE-16GB</span><br><span class="line">支持全局内存L1缓存</span><br><span class="line">L2缓存大小：4M</span><br></pre></td></tr></table></figure>

<h1 id="4-8"><a href="#4-8" class="headerlink" title="4.8"></a>4.8</h1><h2 id="计算资源分配"><a href="#计算资源分配" class="headerlink" title="计算资源分配"></a>计算资源分配</h2><h3 id="线程执行资源分配"><a href="#线程执行资源分配" class="headerlink" title="线程执行资源分配"></a>线程执行资源分配</h3><ul>
<li>线程束本地执行上下文主要资源组成:<br>1)程序计数器;<br>2)寄存器;<br>3)共享内存;</li>
<li>SM处理的每个线程束计算所需的计算资源属于片上(on-chip)资源，因此从一个执行上下文切换到另一个执行上下文是没有时间损耗的。</li>
<li>对于一个给定的内核，同时存在于同一个SM中的线程块和线程束数量取决于在SM中可用的内核所需寄存器和共享内存数量</li>
</ul>
<h3 id="寄存器对线程数目的影响"><a href="#寄存器对线程数目的影响" class="headerlink" title="寄存器对线程数目的影响"></a>寄存器对线程数目的影响</h3><ul>
<li>每个线程消耗的寄存器越多，则可以放在一个SM中的线程束就越少</li>
<li>如果减少内核消耗寄存器的数量，SM便可以同时处理更多的线程束<ul>
<li>假设一个SM的寄存器大小为64KB<ul>
<li>一个线程所需寄存器大小为1KB，则同时可供64个线程使用，对应2个线程束</li>
<li>一个线程所需寄存器大小为0.5KB，则同时可供128个线程使用，对应4个线程束</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240410150451090.png" alt="image-20240410150451090"></p>
<h3 id="共享内存对线程块数量的影响"><a href="#共享内存对线程块数量的影响" class="headerlink" title="共享内存对线程块数量的影响"></a>共享内存对线程块数量的影响</h3><ul>
<li>一个线程块消耗的共享内存越多，则在一个SM中可以同时处理的线程块就会变少</li>
<li>如果每个线程块使用的共享内存数量变少，那么可以同时处理更多的线程块</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240410150648112.png" alt="image-20240410150648112"></p>
<h3 id="SM占有率"><a href="#SM占有率" class="headerlink" title="SM占有率"></a>SM占有率</h3><ul>
<li><p>当计算资源(如寄存器和共享内存)已分配给线程块时，线程块被称为活跃的块，线程块所包含的线程束被称为活跃的线程束，活跃线程束可分为以下3种类型:</p>
<ul>
<li>1)选定的线程束<ul>
<li>活跃的且正在执行的线程束</li>
</ul>
</li>
<li>2)阻塞的线程束<ul>
<li>尚未做好执行的准备的线程束</li>
</ul>
</li>
<li>3)符合条件的线程束。<ul>
<li>活跃的但尚未执行的线程束</li>
</ul>
</li>
</ul>
</li>
<li><p>占用率是每个SM中活跃的线程束占最大线程束的比值:</p>
<ul>
<li>占用率=活跃线程束数量/最大线程束数量<ul>
<li>最大线程束数量 = SM中最多驻留线程数量/32</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>要最大化利用计算资源，则设置的线程块数量和线程数量要尽可能的和SM中最多驻留线程块数量和SM中最多驻留线程数量一致</strong></p>
</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240410151421799.png" alt="image-20240410151421799"></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"common.cuh"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;    </span><br><span class="line">    <span class="keyword">int</span> devID = <span class="number">0</span>;</span><br><span class="line">    cudaDeviceProp deviceProps;</span><br><span class="line">    CUDA_CHECK(cudaGetDeviceProperties(&amp;deviceProps, devID));</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"运行GPU设备:"</span> &lt;&lt; deviceProps.name &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"SM数量："</span> &lt;&lt; deviceProps.multiProcessorCount &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"L2缓存大小："</span> &lt;&lt; deviceProps.l2CacheSize / (<span class="number">1024</span> * <span class="number">1024</span>) &lt;&lt; <span class="string">"M"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"SM最大驻留线程数量："</span> &lt;&lt; deviceProps.maxThreadsPerMultiProcessor &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"设备是否支持流优先级："</span> &lt;&lt; deviceProps.streamPrioritiesSupported &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"设备是否支持在L1缓存中缓存全局内存："</span> &lt;&lt; deviceProps.globalL1CacheSupported &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"设备是否支持在L1缓存中缓存本地内存："</span> &lt;&lt; deviceProps.localL1CacheSupported &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"一个SM可用的最大共享内存量："</span> &lt;&lt; deviceProps.sharedMemPerMultiprocessor / <span class="number">1024</span>  &lt;&lt; <span class="string">"KB"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"一个SM可用的32位最大寄存器数量："</span> &lt;&lt; deviceProps.regsPerMultiprocessor / <span class="number">1024</span> &lt;&lt; <span class="string">"K"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"一个SM最大驻留线程块数量："</span> &lt;&lt; deviceProps.maxBlocksPerMultiProcessor &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"GPU内存带宽："</span> &lt;&lt; deviceProps.memoryBusWidth &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"GPU内存频率："</span> &lt;&lt; (<span class="keyword">float</span>)deviceProps.memoryClockRate / (<span class="number">1024</span> * <span class="number">1024</span>) &lt;&lt; <span class="string">"GHz"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    CUDA_CHECK(cudaDeviceReset());</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<ul>
<li>计算能力8.9为例:<ul>
<li>一个SM最多拥有的线程块个数为Nb=24</li>
<li>一个SM最多拥有的线程个数为Nt=1536</li>
<li>一个SM寄存器大小为64KB</li>
<li>一个SM共享内存上限为100KB</li>
<li>一个线程块的共享内存上限为99KB</li>
</ul>
</li>
<li>并行性规模足够大(即核函数执行配置中定义的总线程足够多)的前提下分析SM占有率:<ul>
<li>1)寄存器和共享内存使用很少的情况，线程块中线程数不小于64(Nt/Nb)时，可以获得100%的占有率<ul>
<li>此时设置的线程数量=线程块（24）* 线程块大小（64） = 1536， 正好等于Nt</li>
<li>SM占有率 = (1536/32)/(Nt/32) = 100%  </li>
</ul>
</li>
<li>2)有限寄存器对占有率的影响，当在SM上驻留最多的线程1536个，核函数中每个线程最多使用42个寄存器<ul>
<li>每个线程最多使用寄存器数量 = 64KB / 1536 = 65536 / 1536 = 42</li>
<li>当一个线程所需的寄存器数量大于42时，可驻留的线程数就会小于1536， 所以SM占有率就会小于100%</li>
<li>例如当一个线程所需的寄存器数量大于84时，可驻留的线程数就会小于768，SM占有率就会小于50%</li>
</ul>
</li>
<li>3)有限共享内存对占有率的影响，若线程块大小定义为64，每个SM需要激活24个线程块才能拥有1536个线程，达到100%的利用率每个线程块可分配4.16KB的共享内存。<ul>
<li>一个线程块可分配的共享内存大小 = 一个SM共享内存上限（100KB） / 线程块数量（24） = 4.16</li>
<li>如果一个线程块所需的共享内存大小大于8.32，可用的线程块就会小于12， SM占有率也会小于50%</li>
</ul>
</li>
</ul>
</li>
<li>注意:如果一个线程块需要使用的共享内存超过了99KB，会导致核函数无法启动。</li>
<li><strong>网格和线程块大小的准则</strong>:<ul>
<li>1)保持每个线程块中线程数量是线程束大小的倍数, 即32的倍数<ul>
<li>若线程块中线程数量为65，则需要3个线程束，第三个线程束中却只有一个线程活跃，剩余31个线程不工作，就会导致资源浪费</li>
</ul>
</li>
<li>2)线程块不要设计的太小;<ul>
<li>最好大于等于一个SM最多拥有的线程块个数</li>
</ul>
</li>
<li>3)根据内核资源（寄存器和共享内存大小）调整线程块的大小</li>
<li>4)线程块的数量要远远大于SM的数量，保证设备有足够的并行<ul>
<li>为了实现延迟隐藏</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="4-9"><a href="#4-9" class="headerlink" title="4.9"></a>4.9</h1><h2 id="延迟隐藏"><a href="#延迟隐藏" class="headerlink" title="延迟隐藏"></a>延迟隐藏</h2><h3 id="延迟隐藏的概念"><a href="#延迟隐藏的概念" class="headerlink" title="延迟隐藏的概念"></a>延迟隐藏的概念</h3><ul>
<li>指令延迟:在指令发出和完成之间的时钟周期被定义为指令延迟<ul>
<li>例如一个指令的时钟周期为5，在指令发出后需要等待5个时钟周期才能完成，这个等待的过程就是指令延迟</li>
</ul>
</li>
<li>当每个时钟周期中所有线程束调度器都有一个符合条件的线程束时，可以达到计算资源的完全利用<ul>
<li>例如有5个指令，每个指令时钟周期都为5，则第一个指令发出后，接着发第二个，一共发五次，第五个指令发出后正好第一个指令完成，对于第一个指令来说就没有等待的过程了</li>
</ul>
</li>
<li>GPU的指令延迟被其他线程束的计算隐藏称为延迟隐藏</li>
<li>指令可以被分为两种基本类型<ul>
<li>1)算数指令</li>
<li>2)内存指令</li>
</ul>
</li>
</ul>
<h3 id="算术指令隐藏"><a href="#算术指令隐藏" class="headerlink" title="算术指令隐藏"></a>算术指令隐藏</h3><ul>
<li>算数运算指令延迟是从开始运算到得到计算结果的时钟周期，通常为4个时钟周期</li>
<li>满足延迟隐藏所需的线程束数量，利用利特尔法则可以合理提供一个估计值:<ul>
<li>所需线程束数量=延迟 x吞吐量</li>
<li>吞吐量是实际数据访问速度</li>
<li>带宽是理论数据访问速度</li>
</ul>
</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240410155730093.png" alt="image-20240410155730093"></p>
<ul>
<li><p>算数运算指令延迟是从开始运算到得到计算结果的时钟周期，通常为4个时钟周期</p>
</li>
<li><p>吞吐量是SM中每个时钟周期的操作数量确定的</p>
</li>
<li><p>16-bit 所需线程束数量=512/32=16 （需要512个线程才能隐藏延迟）</p>
</li>
<li><p>32-bit 所需线程束数量=512/32=16</p>
</li>
<li><p>64-bit 所需线程束数量=8/32=1(8个操作也需要1个线程束)</p>
</li>
<li><p>提升算数指令并行性方法:</p>
<ul>
<li>1)线程中更多独立指令</li>
<li>2)更多并发线程</li>
</ul>
</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240410160418026.png" alt="image-20240410160418026"></p>
<h3 id="内存指令隐藏"><a href="#内存指令隐藏" class="headerlink" title="内存指令隐藏"></a>内存指令隐藏</h3><ul>
<li>内存访问指令延迟是从命令发出到数据到达目的地的时钟周期，通常为400~800个时钟周期</li>
<li>对内存操作来说，其所需的并行可以表示为在每个时钟周期内隐藏内存延迟所需的字节数<ul>
<li>504.2G/s / 10.0145GHz = 50B/cycle</li>
<li>800x50 / 1024=39KB</li>
</ul>
</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240410161523872.png" alt="image-20240410161523872"></p>
<ul>
<li>假设每个线程都把一个浮点数(4字节)从全局内存移动到SM中进行计算，则至少需要10000<br>线程或者313个线程束来隐藏所有内存延迟<ul>
<li>39KB / 4 = 10000个线程</li>
<li>10000个线程  / 32个线程 = 313个线程束</li>
</ul>
</li>
</ul>
<h1 id="4-10"><a href="#4-10" class="headerlink" title="4.10"></a>4.10</h1><h2 id="避免线程束分化"><a href="#避免线程束分化" class="headerlink" title="避免线程束分化"></a>避免线程束分化</h2><h3 id="线程束分支"><a href="#线程束分支" class="headerlink" title="线程束分支"></a>线程束分支</h3><ul>
<li>GPU支持传统的、C/C++风格的显式控制流结构，如if…then…else for和while;<ul>
<li>核函数中通过if…then…else控制不同的条件下的线程执行不同逻辑的任务，就会导致线程任务的不同，导致线程束分化</li>
</ul>
</li>
<li>GPU是相对简单的设备，没有复杂的分支预测机制</li>
<li>一个线程束中的所有线程在同一个周期中必须执行相同的指令</li>
<li>如果同一个线程束中的线程执行不同分支的指令，则会造成线程束分支</li>
<li>下图紫色表示等待的线程<ul>
<li>32个线程中，满足if的部分会同时并行执行，但是满足else只能等待，就会导致资源浪费</li>
<li>else部分需要在if的部分执行完成后才执行，此时if部分的线程又处于等待，进一步导致资源浪费</li>
</ul>
</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240410162455275.png" alt="image-20240410162455275"></p>
<ul>
<li>线程束分支会降低GPU的并行计算能力，条件分支越多，并行性削弱越严重，</li>
<li>线程束分支只发生在同一个线程束中，不同线程束不会发生线程束分化<ul>
<li>因此可以让不同的线程束负责不同的任务（同一个线程束中所有线程的任务类型相同），而不是在一个线程束中让不同的线程负责不同的任务</li>
</ul>
</li>
<li>为获取最佳性能，应避免在同一个线程束中有不同的执行路径</li>
<li>下面的代码是对上面代码的优化<ul>
<li>通过线程束的编号来控制不同线程束负责不同的逻辑任务</li>
</ul>
</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240410162951128.png" alt="image-20240410162951128"></p>
<h3 id="并行规约计算"><a href="#并行规约计算" class="headerlink" title="并行规约计算"></a>并行规约计算</h3><ul>
<li>在向量中满足交换律和结合律的运算，称为规约问题，并行执行的规约计算称为并行规约计算</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240410163654654.png" alt="image-20240410163654654"></p>
<ul>
<li>假设要计算4096个元素求和，设计线程块大小为512，每个线程负责一个数据元素，共需8个线程块。<ul>
<li>GPU负责8个线程块的求和运算</li>
<li>将8个线程块的求个结果复制到主机中串行求和</li>
</ul>
</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240410163838655.png" alt="image-20240410163838655"></p>
<ul>
<li>如下是两种不同的并行规约计算方式</li>
</ul>
<p><img src="/2024/04/03/CUDA/CUDA/image-20240410164028911.png" alt="image-20240410164028911"></p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><blockquote>
<p>github</p>
</blockquote>
<p>编译</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cmake -S . -B build</span><br><span class="line">cmake --build build/</span><br></pre></td></tr></table></figure>


    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CUDA/" rel="tag"># CUDA</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/01/26/C++/C++important/" rel="prev" title="C++重点">
      <i class="fa fa-chevron-left"></i> C++重点
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/11/04/frontend/frontend/" rel="next" title="frontend/frontend">
      frontend/frontend <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#0"><span class="nav-text">0</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#windows的WSL安装Ubuntu"><span class="nav-text">windows的WSL安装Ubuntu</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-2"><span class="nav-text">1.2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA下载安装"><span class="nav-text">CUDA下载安装</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-3"><span class="nav-text">1.3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#nvidia-smi"><span class="nav-text">nvidia-smi</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装显卡驱动"><span class="nav-text">安装显卡驱动</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ubuntu安装g"><span class="nav-text">ubuntu安装g++</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nvcc-fatal错误"><span class="nav-text">nvcc fatal错误</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-2"><span class="nav-text">2.2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#核函数"><span class="nav-text">核函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-3"><span class="nav-text">2.3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#线程模型"><span class="nav-text">线程模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一维"><span class="nav-text">一维</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多维"><span class="nav-text">多维</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-4"><span class="nav-text">2.4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#线程全局索引计算方式"><span class="nav-text">线程全局索引计算方式</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-5"><span class="nav-text">2.5</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#nvcc编译流程与GPU计算能力"><span class="nav-text">nvcc编译流程与GPU计算能力</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#nvcc编译流程"><span class="nav-text">nvcc编译流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PTX"><span class="nav-text">PTX</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GPU架构与计算能力"><span class="nav-text">GPU架构与计算能力</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-6"><span class="nav-text">2.6</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA程序兼容性问题"><span class="nav-text">CUDA程序兼容性问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#指定虚拟架构计算能力"><span class="nav-text">指定虚拟架构计算能力</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#指定真实架构计算能力"><span class="nav-text">指定真实架构计算能力</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#指定多个GPU版本编译"><span class="nav-text">指定多个GPU版本编译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nvcc即时编译"><span class="nav-text">nvcc即时编译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nvcc编译默认计算能力"><span class="nav-text">nvcc编译默认计算能力</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-1"><span class="nav-text">3.1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA矩阵加法运算程序"><span class="nav-text">CUDA矩阵加法运算程序</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CUDA程序基本框架"><span class="nav-text">CUDA程序基本框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#设置GPU设备"><span class="nav-text">设置GPU设备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#内存管理"><span class="nav-text">内存管理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#内存分配"><span class="nav-text">内存分配</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数据拷贝"><span class="nav-text">数据拷贝</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#内存初始化"><span class="nav-text">内存初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#内存释放"><span class="nav-text">内存释放</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#自定义设备函数"><span class="nav-text">自定义设备函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-2"><span class="nav-text">3.2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA错误检测"><span class="nav-text">CUDA错误检测</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一、运行时API错误代码"><span class="nav-text">一、运行时API错误代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二、错误检查函数"><span class="nav-text">二、错误检查函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1、涉及的运行时API函数介绍"><span class="nav-text">1、涉及的运行时API函数介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2、代码解析"><span class="nav-text">2、代码解析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3、实例演示"><span class="nav-text">3、实例演示</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#三、检查核函数"><span class="nav-text">三、检查核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1、涉及的运行时API函数介绍-1"><span class="nav-text">1、涉及的运行时API函数介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2、捕捉核函数错误方法"><span class="nav-text">2、捕捉核函数错误方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3、实例演示-1"><span class="nav-text">3、实例演示</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-3"><span class="nav-text">3.3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA计时"><span class="nav-text">CUDA计时</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一、事件记时"><span class="nav-text">一、事件记时</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1、记时代码解析"><span class="nav-text">1、记时代码解析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2、核函数记时实例演示"><span class="nav-text">2、核函数记时实例演示</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二、nvprof性能刨析"><span class="nav-text">二、nvprof性能刨析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1、nvprof工具说明"><span class="nav-text">1、nvprof工具说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2、nvprof实例演示"><span class="nav-text">2、nvprof实例演示</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-4"><span class="nav-text">3.4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#运行时GPU信息查询"><span class="nav-text">运行时GPU信息查询</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一、运行时API查询GPU信息"><span class="nav-text">一、运行时API查询GPU信息</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1、涉及的运行时API函数介绍-2"><span class="nav-text">1、涉及的运行时API函数介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2、实例演示"><span class="nav-text">2、实例演示</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二、查询GPU计算核心数量"><span class="nav-text">二、查询GPU计算核心数量</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-5"><span class="nav-text">3.5</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#组织线程模型"><span class="nav-text">组织线程模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数据存储方式"><span class="nav-text">数据存储方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二维网格二维线程块"><span class="nav-text">二维网格二维线程块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二维网格一维线程块"><span class="nav-text">二维网格一维线程块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一维网格一维线程块"><span class="nav-text">一维网格一维线程块</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-1"><span class="nav-text">4.1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GPU硬件资源"><span class="nav-text">GPU硬件资源</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#流多处理器–SM"><span class="nav-text">流多处理器–SM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线程模型与物理结构"><span class="nav-text">线程模型与物理结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线程束"><span class="nav-text">线程束</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-2"><span class="nav-text">4.2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA内存模型概述"><span class="nav-text">CUDA内存模型概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#内存结构层次特点"><span class="nav-text">内存结构层次特点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CUDA内存模型"><span class="nav-text">CUDA内存模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-3"><span class="nav-text">4.3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#寄存器和本地内存"><span class="nav-text">寄存器和本地内存</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#寄存器"><span class="nav-text">寄存器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#本地内存"><span class="nav-text">本地内存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#寄存器溢出"><span class="nav-text">寄存器溢出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实例"><span class="nav-text">实例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-4"><span class="nav-text">4.4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#全局内存"><span class="nav-text">全局内存</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#全局变量初始化"><span class="nav-text">全局变量初始化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-5"><span class="nav-text">4.5</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#共享内存"><span class="nav-text">共享内存</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#共享内存作用"><span class="nav-text">共享内存作用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#静态共享内存"><span class="nav-text">静态共享内存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#动态共享内存"><span class="nav-text">动态共享内存</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-6"><span class="nav-text">4.6</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#常量内存"><span class="nav-text">常量内存</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-7"><span class="nav-text">4.7</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GPU缓存"><span class="nav-text">GPU缓存</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GPU缓存种类"><span class="nav-text">GPU缓存种类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GPU缓存作用"><span class="nav-text">GPU缓存作用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L1缓存查询与设置"><span class="nav-text">L1缓存查询与设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L1缓存与共享内存"><span class="nav-text">L1缓存与共享内存</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-8"><span class="nav-text">4.8</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#计算资源分配"><span class="nav-text">计算资源分配</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线程执行资源分配"><span class="nav-text">线程执行资源分配</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#寄存器对线程数目的影响"><span class="nav-text">寄存器对线程数目的影响</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#共享内存对线程块数量的影响"><span class="nav-text">共享内存对线程块数量的影响</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SM占有率"><span class="nav-text">SM占有率</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-9"><span class="nav-text">4.9</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#延迟隐藏"><span class="nav-text">延迟隐藏</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#延迟隐藏的概念"><span class="nav-text">延迟隐藏的概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算术指令隐藏"><span class="nav-text">算术指令隐藏</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#内存指令隐藏"><span class="nav-text">内存指令隐藏</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-10"><span class="nav-text">4.10</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#避免线程束分化"><span class="nav-text">避免线程束分化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线程束分支"><span class="nav-text">线程束分支</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#并行规约计算"><span class="nav-text">并行规约计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代码"><span class="nav-text">代码</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Chuckie"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Chuckie</p>
  <div class="site-description" itemprop="description">Bright future</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">116</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">100</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ChuckieWill" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ChuckieWill" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/codewyj.163.com" title="E-Mail → codewyj.163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.jianshu.com/u/de627226656c" title="https:&#x2F;&#x2F;www.jianshu.com&#x2F;u&#x2F;de627226656c" rel="noopener" target="_blank">简书</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="/images/me.png"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chuckie</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.1m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">17:04</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
